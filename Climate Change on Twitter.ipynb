{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter's Climate Tide: An Analysis of Climate Change Sentiment on Twitter\n",
    "### By Arjun Gandhi\n",
    "### Last updated: December 10, 2020\n",
    "### THIS IS A DRAFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "I am starting off with a data set from Harvard that contains 39.6 million tweets related to climate change. The data set is in tweet IDs (numbers) so I need get the tweets for each tweet ID.\n",
    "\n",
    "Here is the link the data set: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU\n",
    "\n",
    "As states in the above link the data is from September 21, 2017 and May 17, 2019 and they had a gap in data collection from January 7, 2019 to April 17, 2019.\n",
    "\n",
    "To convert each tweet ID into the actual tweet data I am using this: Hydrator [Computer Software]. Retrieved from https://github.com/docnow/hydrator\n",
    "\n",
    "From the above repo, I downloaded this version of the app: https://github.com/DocNow/hydrator/releases/tag/v0.0.13\n",
    "\n",
    "The tweets are seperated by file (~ 10 million tweets/file). I made a Twitter account to connect my account this Hydrator. I then uploaded each txt file into Hydrator under \"Datasets\" in the desktop app. \n",
    "\n",
    "TALK ABOUT THEIR METHODOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>media</th>\n",
       "      <th>urls</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_listed_count</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name.1</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_time_zone</th>\n",
       "      <th>user_urls</th>\n",
       "      <th>user_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>UN cdnpoli ONpoli ABpoli</td>\n",
       "      <td>https://twitter.com/TheRebelTV/status/10722948...</td>\n",
       "      <td>https://www.therebel.media/un-global-warming-m...</td>\n",
       "      <td>92</td>\n",
       "      <td>1072294898588631040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>203482</td>\n",
       "      <td>17522</td>\n",
       "      <td>1243</td>\n",
       "      <td>Canada and the world</td>\n",
       "      <td>Rebel News</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>38905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rebelnews.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>955376892026093569</td>\n",
       "      <td>Pontifex</td>\n",
       "      <td>9.551606e+17</td>\n",
       "      <td>500704345.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Sep 17 04:42:16 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://truthout.org/articles/national-park-of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1041547863795224576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2065</td>\n",
       "      <td>2383</td>\n",
       "      <td>98</td>\n",
       "      <td>USA</td>\n",
       "      <td>OurRevolution</td>\n",
       "      <td>LeftysUnite</td>\n",
       "      <td>49972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>Spain Portugal climatechange globalwarming Hea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://news.sky.com/story/live-scorching-satu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1025728615399469058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Hugh</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stevenhugh.wordpress.com/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>Resist FakePresident Dontard GOP NRA War Clima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/mattmfm/status/93272970237...</td>\n",
       "      <td>0</td>\n",
       "      <td>932915956824682496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6191</td>\n",
       "      <td>6720</td>\n",
       "      <td>121</td>\n",
       "      <td>the beautiful \"Jemez\" USA</td>\n",
       "      <td>Athoughtz</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>155930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://TokTok.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  coordinates                      created_at  \\\n",
       "0         NaN  Tue Dec 11 01:00:00 +0000 2018   \n",
       "1         NaN  Mon Jan 22 09:49:35 +0000 2018   \n",
       "2         NaN  Mon Sep 17 04:42:16 +0000 2018   \n",
       "3         NaN  Sat Aug 04 13:02:13 +0000 2018   \n",
       "4         NaN  Tue Nov 21 10:17:51 +0000 2017   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                           UN cdnpoli ONpoli ABpoli   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Spain Portugal climatechange globalwarming Hea...   \n",
       "4  Resist FakePresident Dontard GOP NRA War Clima...   \n",
       "\n",
       "                                               media  \\\n",
       "0  https://twitter.com/TheRebelTV/status/10722948...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                urls  favorite_count  \\\n",
       "0  https://www.therebel.media/un-global-warming-m...              92   \n",
       "1                                                NaN               0   \n",
       "2  https://truthout.org/articles/national-park-of...               0   \n",
       "3  https://news.sky.com/story/live-scorching-satu...               1   \n",
       "4  https://twitter.com/mattmfm/status/93272970237...               0   \n",
       "\n",
       "                    id in_reply_to_screen_name  in_reply_to_status_id  \\\n",
       "0  1072294898588631040                     NaN                    NaN   \n",
       "1   955376892026093569                Pontifex           9.551606e+17   \n",
       "2  1041547863795224576                     NaN                    NaN   \n",
       "3  1025728615399469058                     NaN                    NaN   \n",
       "4   932915956824682496                     NaN                    NaN   \n",
       "\n",
       "   in_reply_to_user_id  ... user_followers_count user_friends_count  \\\n",
       "0                  NaN  ...               203482              17522   \n",
       "1          500704345.0  ...                    1                  4   \n",
       "2                  NaN  ...                 2065               2383   \n",
       "3                  NaN  ...                   25                 24   \n",
       "4                  NaN  ...                 6191               6720   \n",
       "\n",
       "  user_listed_count              user_location      user_name  \\\n",
       "0              1243       Canada and the world     Rebel News   \n",
       "1                 0              United States          Frank   \n",
       "2                98                        USA  OurRevolution   \n",
       "3                 1                        NaN    Steven Hugh   \n",
       "4               121  the beautiful \"Jemez\" USA      Athoughtz   \n",
       "\n",
       "   user_screen_name.1 user_statuses_count user_time_zone  \\\n",
       "0     RebelNewsOnline               38905            NaN   \n",
       "1       Frank34802901                 100            NaN   \n",
       "2         LeftysUnite               49972            NaN   \n",
       "3         Steven9Hugh                1402            NaN   \n",
       "4           athoughtz              155930            NaN   \n",
       "\n",
       "                           user_urls user_verified  \n",
       "0          https://www.rebelnews.com          True  \n",
       "1                                NaN         False  \n",
       "2                                NaN         False  \n",
       "3  https://stevenhugh.wordpress.com/         False  \n",
       "4                  http://TokTok.com         False  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./data/tweets200k.csv\")\n",
    "data = data.head(5000) # REMOVE THIS LINE !!!!!!!!!!!!!!\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "The data set has lots of data that is not needed for this analysis. Since we are looking at sentiment over time and other factors related to polticis of a state and events, it is simplest to just drop all non-English tweets.\n",
    "\n",
    "There are lots of extranenous columns that are not relavent to this project so I just dropped them. These include things like user specifics like their profile details and other things like the URL of thr tweet or the language since all will be English. \n",
    "\n",
    "I then renamed some columns for my ease of use of the data set and switched the tweet ID to be the index columns.\n",
    "\n",
    "The date and time is given as a string so I use regular expressions to convert that to a date time object. The hashtag column is given as one string so I split that into a list of hastags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English tweets from the data set\n",
    "data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "# Drop all the unneeded columns from the data set\n",
    "cols_to_delete = [\"user_urls\", \"user_statuses_count\", \"coordinates\", \"user_name\", \"in_reply_to_status_id\", \n",
    "                  \"in_reply_to_user_id\", \"user_time_zone\", \"urls\", \"lang\", \"media\", \"source\", \n",
    "                  \"retweet_screen_name\", \"retweet_id\", \"possibly_sensitive\", \"tweet_url\",\n",
    "                  \"user_default_profile_image\", \"user_friends_count\", \"user_verified\", \"user_location\", \n",
    "                   \"in_reply_to_screen_name\", \"user_screen_name.1\",\n",
    "                  \"user_favourites_count\", \"user_listed_count\", \"user_created_at\", \"user_description\", \"place\", \n",
    "                 \"user_followers_count\", \"hashtags\", \"favorite_count\", \"retweet_count\"]\n",
    "\n",
    "data = data.drop(columns=cols_to_delete)\n",
    "\n",
    "# Swap the index column from 0...n to the tweet ID and rename the column from id to tweetID and rename to clarify\n",
    "# column meaning\n",
    "data = data.rename(columns={\"id\": \"tweetID\", \"created_at\": \"date/time\", \"user_screen_name\": \"tweeter\"})\n",
    "data = data.set_index('tweetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dates time strings into datetime objects\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "dates = []\n",
    "\n",
    "# Matching this text\n",
    "# Mon Jan 22 09:49:35 +0000 2018\n",
    "# For every row in the dataframe\n",
    "regex = re.compile(r\"(\\w{3}) (\\w{3}) (\\d\\d) (\\d\\d:\\d\\d:\\d\\d) \\+(0{4}) (\\d{4})\")\n",
    "\n",
    "# Given a string of a month return the corresponding integer for that month i.e. Jan == 1\n",
    "def numerize(str):\n",
    "    month = str.lower()\n",
    "    if (month == \"jan\"): return 1\n",
    "    elif (month == \"feb\"): return 2\n",
    "    elif (month == \"mar\"): return 3\n",
    "    elif (month == \"apr\"): return 4\n",
    "    elif (month == \"may\"): return 5\n",
    "    elif (month == \"jun\"): return 6\n",
    "    elif (month == \"jul\"): return 7 \n",
    "    elif (month == \"aug\"): return 8\n",
    "    elif (month == \"sep\"): return 9\n",
    "    elif (month == \"oct\"): return 10\n",
    "    elif (month == \"nov\"): return 11\n",
    "    elif (month == \"dec\"): return 12\n",
    "        \n",
    "for row in data.iterrows():\n",
    "    dt = row[1][\"date/time\"]\n",
    "    matches = re.search(regex, dt)\n",
    "    groups = matches.groups()    \n",
    "    month = numerize(groups[1])\n",
    "    d = datetime.date(int(groups[5]), month, int(groups[2]))\n",
    "    dates.append(d)\n",
    "    \n",
    "data = data.drop(columns=[\"date/time\"])\n",
    "data[\"date_tweeted\"] = dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tweets for sentiment analysis\n",
    "There are several things that need to be done to the actual text of the tweets before we can do sentiment analysis on them. To starts of, I will do some basic things like make all tweet bodies lower case so that words like CLIMATE and climate and cLiMate are all treated the same by the model I use later on. Next, I am going to remove all links from these tweets because that is irrelvanet to the sentiment of the tweet. There are many things like this that I will do here then I will move on to make the tweets \"linguistically sound\" for analysis by doing things like removing words without meaning that won't contribute the analysis and then making all words their base word or lemmatizing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all tweets lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all tweets lower case so that when use a model to look at sentiment words that are the same are treated\n",
    "# so by the model i.e. capitalization won't make the model think Word is not word.\n",
    "lower_case_tweets = []\n",
    "for r in data.iterrows(): lower_case_tweets.append(r[1][\"text\"].lower())\n",
    "data = data.drop(columns=[\"text\"])\n",
    "data[\"text\"] = lower_case_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove links from tweets\n",
    "Links are pretty useless if I want to know what the text of a tweet means. I could have gone a step further and taken the links and scraped it and then looked at that sentiment but for simplicty I omitted this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkless = []\n",
    "regex = re.compile(r\"http\\S+\")\n",
    "\n",
    "# remove all links from each tweet\n",
    "for row in data.iterrows():\n",
    "    txt = row[1][\"text\"]\n",
    "    if txt.find(\"https://t.co\"): \n",
    "        ll = re.sub(regex, \"\", txt)\n",
    "        linkless.append(ll)\n",
    "    else: \n",
    "        linkless.append(txt)\n",
    "\n",
    "data[\"text\"] = linkless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove puncuation and digits\n",
    "Puncuation does not really add much when doing sentiment analysis and so I will remove puncuation from the tweets. I will also remove tokens (words or digits) in the tweets that contain digits so words with a digit in it or a digit from the tweet for similar reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "import string\n",
    "\n",
    "# Remove all puncuation\n",
    "data['text']=data['text'].apply(lambda tweet: tweet.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Remove digits\n",
    "data['text']=data['text'].apply(lambda tweet: re.sub(\"\\w*\\d\\w*\",\"\", tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and Lemmatization \n",
    "Here I will remove stopwards from the tweet bodies. These are words like \"I\" and \"this\" that add little meaning to the tweet but if left in the text will give me an innacurate depiction of the most common words in the tweets. Thne I will perform lemmatization on the tweets. This just means taking words that linguisticlly mean the same thing like walker and walking and reducing them to their base. In this case the word walk. You can read more here: https://en.wikipedia.org/wiki/Lemmatisation. I will be doing this uisng spaCy (https://spacy.io).\n",
    "\n",
    "You can find installation instructions for spaCy here: https://spacy.io/usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did these commands to setup spaCy.\n",
    "!pip install -U spacy\n",
    "!pip install -U spacy-lookups-data\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweeter</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>therebeltv go different un conference â€” medium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>pontifex prayer   god amp true father     jesu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>red alert spain portugal europe near alltime h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>athoughtz</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>trump gop swamp resist fakepresident dontard g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>study green building save   billion health amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120089046217306112</th>\n",
       "      <td>JillianDURM</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>hope folk fabulous demand win big</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069679067065139203</th>\n",
       "      <td>goldenerin</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>good thing planet human extinct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946759916425039872</th>\n",
       "      <td>MagnaCartaRules</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>cillizzacnn global warming advocate fact argue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128017194984587264</th>\n",
       "      <td>riversidecrew</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>europeangreen sufficient vote difference polit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949954734600765440</th>\n",
       "      <td>Sheoakbloke1</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>freeradicalone bbfromcanberra australias gg ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4568 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tweeter date_tweeted  \\\n",
       "tweetID                                             \n",
       "1072294898588631040  RebelNewsOnline   2018-12-11   \n",
       "955376892026093569     Frank34802901   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh   2018-08-04   \n",
       "932915956824682496         athoughtz   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg   2018-09-17   \n",
       "...                              ...          ...   \n",
       "1120089046217306112      JillianDURM   2019-04-21   \n",
       "1069679067065139203       goldenerin   2018-12-03   \n",
       "946759916425039872   MagnaCartaRules   2017-12-29   \n",
       "1128017194984587264    riversidecrew   2019-05-13   \n",
       "949954734600765440      Sheoakbloke1   2018-01-07   \n",
       "\n",
       "                                                                  text  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  therebeltv go different un conference â€” medium...  \n",
       "955376892026093569   pontifex prayer   god amp true father     jesu...  \n",
       "1025728615399469058  red alert spain portugal europe near alltime h...  \n",
       "932915956824682496   trump gop swamp resist fakepresident dontard g...  \n",
       "1041547806622797824  study green building save   billion health amp...  \n",
       "...                                                                ...  \n",
       "1120089046217306112                  hope folk fabulous demand win big  \n",
       "1069679067065139203                    good thing planet human extinct  \n",
       "946759916425039872   cillizzacnn global warming advocate fact argue...  \n",
       "1128017194984587264  europeangreen sufficient vote difference polit...  \n",
       "949954734600765440   freeradicalone bbfromcanberra australias gg ca...  \n",
       "\n",
       "[4568 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model \n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Lemmatization of tweets and stopwords removal\n",
    "data['text']=data['text'].apply(lambda t: ' '.join([tok.lemma_ for tok in list(model(t)) if (tok.is_stop==False)]))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bag of words of the tweets\n",
    "I create a bag of words of the tweets using CountVectorizer from sklearn. This doesn't look like it means anything, it literally just looks like a matrix of 0s...but will be super helpful in making word clouds so we can better visualize these tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaas</th>\n",
       "      <th>aaasmtg</th>\n",
       "      <th>aarneclimate</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abate</th>\n",
       "      <th>abatement</th>\n",
       "      <th>abbot</th>\n",
       "      <th>...</th>\n",
       "      <th>ğ€ğ§ğ­ğ¡ğ«ğ¨ğ©ğ¨ğ ğğ§ğ¢ğœ</th>\n",
       "      <th>ğ‚ğ¡ğšğ§ğ ğ</th>\n",
       "      <th>ğ‚ğ¡ğğœğ¤ğ¢ğ§ğ </th>\n",
       "      <th>ğ‚ğ¥ğšğ¢ğ¦</th>\n",
       "      <th>ğ‚ğ¥ğ¢ğ¦ğšğ­ğ</th>\n",
       "      <th>ğ‚ğ¨ğ§ğ¬ğğ§ğ¬ğ®ğ¬</th>\n",
       "      <th>ğ…ğšğœğ­</th>\n",
       "      <th>ğğŸ</th>\n",
       "      <th>ğğ§</th>\n",
       "      <th>ğ“ğ¡ğ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120089046217306112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069679067065139203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946759916425039872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128017194984587264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949954734600765440</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4568 rows Ã— 12321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aaas  aaasmtg  aarneclimate  aaron  abacus  abandon  \\\n",
       "tweetID                                                                    \n",
       "1072294898588631040     0        0             0      0       0        0   \n",
       "955376892026093569      0        0             0      0       0        0   \n",
       "1025728615399469058     0        0             0      0       0        0   \n",
       "932915956824682496      0        0             0      0       0        0   \n",
       "1041547806622797824     0        0             0      0       0        0   \n",
       "...                   ...      ...           ...    ...     ...      ...   \n",
       "1120089046217306112     0        0             0      0       0        0   \n",
       "1069679067065139203     0        0             0      0       0        0   \n",
       "946759916425039872      0        0             0      0       0        0   \n",
       "1128017194984587264     0        0             0      0       0        0   \n",
       "949954734600765440      0        0             0      0       0        0   \n",
       "\n",
       "                     abandons  abate  abatement  abbot  ...  ğ€ğ§ğ­ğ¡ğ«ğ¨ğ©ğ¨ğ ğğ§ğ¢ğœ  \\\n",
       "tweetID                                                 ...                  \n",
       "1072294898588631040         0      0          0      0  ...              0   \n",
       "955376892026093569          0      0          0      0  ...              0   \n",
       "1025728615399469058         0      0          0      0  ...              0   \n",
       "932915956824682496          0      0          0      0  ...              0   \n",
       "1041547806622797824         0      0          0      0  ...              0   \n",
       "...                       ...    ...        ...    ...  ...            ...   \n",
       "1120089046217306112         0      0          0      0  ...              0   \n",
       "1069679067065139203         0      0          0      0  ...              0   \n",
       "946759916425039872          0      0          0      0  ...              0   \n",
       "1128017194984587264         0      0          0      0  ...              0   \n",
       "949954734600765440          0      0          0      0  ...              0   \n",
       "\n",
       "                     ğ‚ğ¡ğšğ§ğ ğ  ğ‚ğ¡ğğœğ¤ğ¢ğ§ğ   ğ‚ğ¥ğšğ¢ğ¦  ğ‚ğ¥ğ¢ğ¦ğšğ­ğ  ğ‚ğ¨ğ§ğ¬ğğ§ğ¬ğ®ğ¬  ğ…ğšğœğ­  ğğŸ  \\\n",
       "tweetID                                                                      \n",
       "1072294898588631040       0         0      0        0          0     0   0   \n",
       "955376892026093569        0         0      0        0          0     0   0   \n",
       "1025728615399469058       0         0      0        0          0     0   0   \n",
       "932915956824682496        0         0      0        0          0     0   0   \n",
       "1041547806622797824       0         0      0        0          0     0   0   \n",
       "...                     ...       ...    ...      ...        ...   ...  ..   \n",
       "1120089046217306112       0         0      0        0          0     0   0   \n",
       "1069679067065139203       0         0      0        0          0     0   0   \n",
       "946759916425039872        0         0      0        0          0     0   0   \n",
       "1128017194984587264       0         0      0        0          0     0   0   \n",
       "949954734600765440        0         0      0        0          0     0   0   \n",
       "\n",
       "                     ğğ§  ğ“ğ¡ğ  \n",
       "tweetID                       \n",
       "1072294898588631040   0    0  \n",
       "955376892026093569    0    0  \n",
       "1025728615399469058   0    0  \n",
       "932915956824682496    0    0  \n",
       "1041547806622797824   0    0  \n",
       "...                  ..  ...  \n",
       "1120089046217306112   0    0  \n",
       "1069679067065139203   0    0  \n",
       "946759916425039872    0    0  \n",
       "1128017194984587264   0    0  \n",
       "949954734600765440    0    0  \n",
       "\n",
       "[4568 rows x 12321 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a bag of words using sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "d=cv.fit_transform(data['text'])\n",
    "bag = pd.DataFrame(d.toarray(), columns=cv.get_feature_names())\n",
    "bag.index=data.index\n",
    "bag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
