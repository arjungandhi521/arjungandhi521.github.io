{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Twitter's Climate Tide: An Analysis of Tweets About Climate Change </center>\n",
    "## <center> By Arjun Gandhi </center>\n",
    "## <center> December 20, 2020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Impacts of Climate Change (Source: NASA)\"](climate_change.jpg)\n",
    "<center>Impacts of Climate Change (https://climate.nasa.gov/effects/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction</center>\n",
    "In this project I will be looking at data from Twitter, specifically tweets collected by George Washing University from 2017 to 2019 with most data from 2018. I will be looking at things like what was the most common things talked about in relation to climate and what the sentiment was for the all these tweets, a specific time, etc. The goal here was to see can I look at some data and see oh I wonder if X event was happening at this time due to something like frequency of the tweet. I originally had in mind doing something like lets look at data from the time of Hurrican Irma and sentiment before and after in a conservative area in FL that was impacted and see if the hurricane caused a change in sentiment to climate change on Twitter. Because most tweets were not geotagged, I did not do this because it would be too difficult to do accurately here. So anyways, lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Getting started </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The libraries I will be using are: [matplotlib](https://matplotlib.org/index.html#), [pandas](https://pandas.pydata.org), [spaCy](https://spacy.io), [seaborn](https://seaborn.pydata.org), [Gensim](https://radimrehurek.com/gensim/), [wordcloud](http://amueller.github.io/word_cloud/), [scikit-learn](https://scikit-learn.org/stable/), and [VADER](https://github.com/cjhutto/vaderSentiment).\n",
    "### Let's install everything I will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "#!pip install wordcloud\n",
    "\n",
    "# Seaborn\n",
    "#!pip install seaborn --upgrade\n",
    "# Gensim\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "# Spacy\n",
    "#!pip install -U spacy\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# VADER\n",
    "#!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import everything needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics and essentials \n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Natural language processing and machine learning\n",
    "import spacy\n",
    "import gensim\n",
    "# Vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Collection </center>\n",
    "I did not go and scrape Twitter to get all tweets that were on climate. Instead, I found a GWU dataset of climate change tweets that were collected from 2017-2019 (Littman, Justin; Wrubel, Laura, 2019, \"Climate Change Tweets Ids\", https://doi.org/10.7910/DVN/5QCCUU, Harvard Dataverse, V1). I downloaded my sample from GWU which makes picking your criteria for the tweets much easier. You can make your own data set from all 40 million right [here](https://tweetsets.library.gwu.edu/datasets).  Here is a link to my exact sample: http://tweetsets.library.gwu.edu/dataset/a66e1b6b. You can also see my sample in the form of the tweet ids in my GitHub repo: [arjungandhi521/arjungandhi521.github.io/public_data/tweets_25k.txt](https://github.com/arjungandhi521/arjungandhi521.github.io/blob/main/public_data/tweets_25k.txt). I sampled 25,000 of the 40 million tweets. The CSV I use directly below is private because of Twitter policy that one should not publish large amounts of tweets but that people in academics and such can go ahead and publisht the tweet ids. As states in the above link the data is from September 21, 2017 and May 17, 2019 and they had a gap in data collection from January 7, 2019 to April 17, 2019. I chose to exclude retweets and this sample ranges from late 2017 to mid-2019 which is the full collection time. I excluded retweets because even though a tweet with 1 billion RTs may have high influence on Twitter, having it n times in the dataset is not really going to add to my understanding of the language. I still have access to the favorites and retweets data which I will use later on. \n",
    "\n",
    "To convert each tweet ID into the actual tweet data I am using Hydrator: Hydrator [Computer Software]. Retrieved from https://github.com/docnow/hydrator. From the above repo, I downloaded [version 0.0.13 of the app](https://github.com/DocNow/hydrator/releases/tag/v0.0.13). I made a Twitter account to connect my account this Hydrator. When you download the sample just pick to download tweet ids. This will be a compressed .txt file. Just unzip it and then upload the file into Hydrator under \"Datasets\" in the desktop app. Then hit \"Add Dataset\" and then \"Start\" and then when its done you can click CSV to get the JSONL as a CSV. When I hydrated the tweets (turned them from tweet ids into tweets you see below), I started with 25,000 ids but the data frame directly below \"only\" has 18,328 tweets. This is for several reasons including people making their accounts private and deleted tweets and accounts. These tweet ids were put into the data set before these thing happend.\n",
    "\n",
    "TALK ABOUT THEIR METHODOLOGY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>media</th>\n",
       "      <th>urls</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_listed_count</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name.1</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_time_zone</th>\n",
       "      <th>user_urls</th>\n",
       "      <th>user_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>UN cdnpoli ONpoli ABpoli</td>\n",
       "      <td>https://twitter.com/TheRebelTV/status/10722948...</td>\n",
       "      <td>https://www.therebel.media/un-global-warming-m...</td>\n",
       "      <td>91</td>\n",
       "      <td>1072294898588631040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>207643</td>\n",
       "      <td>17379</td>\n",
       "      <td>1266</td>\n",
       "      <td>Canada and the world</td>\n",
       "      <td>Rebel News</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>39345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rebelnews.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>955376892026093569</td>\n",
       "      <td>Pontifex</td>\n",
       "      <td>9.551606e+17</td>\n",
       "      <td>5.007043e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Sep 17 04:42:16 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://truthout.org/articles/national-park-of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1041547863795224576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2062</td>\n",
       "      <td>2383</td>\n",
       "      <td>99</td>\n",
       "      <td>USA</td>\n",
       "      <td>OurRevolution</td>\n",
       "      <td>LeftysUnite</td>\n",
       "      <td>50076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>Spain Portugal climatechange globalwarming Hea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://news.sky.com/story/live-scorching-satu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1025728615399469058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Hugh</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stevenhugh.wordpress.com/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>Resist FakePresident Dontard GOP NRA War Clima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/mattmfm/status/93272970237...</td>\n",
       "      <td>0</td>\n",
       "      <td>932915956824682496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6214</td>\n",
       "      <td>6747</td>\n",
       "      <td>121</td>\n",
       "      <td>the beautiful \"Jemez\" USA</td>\n",
       "      <td>Athoughtz</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>155954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://TokTok.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91264</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Apr 27 16:22:49 +0000 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1122174270208126978</td>\n",
       "      <td>bruceanderson</td>\n",
       "      <td>1.122169e+18</td>\n",
       "      <td>8.037439e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>779</td>\n",
       "      <td>9</td>\n",
       "      <td>Regina</td>\n",
       "      <td>sean osmar</td>\n",
       "      <td>SeanOsmar</td>\n",
       "      <td>2229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91265</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Apr 27 11:36:03 +0000 2019</td>\n",
       "      <td>plasticpollution climatechange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1122102103185547270</td>\n",
       "      <td>RochdaleCouncil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.052017e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>177</td>\n",
       "      <td>6</td>\n",
       "      <td>Wandering the Wastelands</td>\n",
       "      <td>Gatt ðŸ‡¬ðŸ‡§ðŸ‡ªðŸ‡º</td>\n",
       "      <td>Gatt_</td>\n",
       "      <td>2591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91266</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Apr 27 16:23:09 +0000 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.usatoday.com/story/news/nation/201...</td>\n",
       "      <td>0</td>\n",
       "      <td>1122174355084062720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>644</td>\n",
       "      <td>2</td>\n",
       "      <td>Omaha, Nebraska</td>\n",
       "      <td>Dee Dee</td>\n",
       "      <td>Dee_Dee2018</td>\n",
       "      <td>2765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91267</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Apr 27 16:23:07 +0000 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1122174343562158081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2217</td>\n",
       "      <td>1838</td>\n",
       "      <td>450</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>Bonnie</td>\n",
       "      <td>greenurlifenow</td>\n",
       "      <td>254477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91268</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Apr 27 16:23:32 +0000 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1122174449095270403</td>\n",
       "      <td>creeker112</td>\n",
       "      <td>1.122173e+18</td>\n",
       "      <td>1.108134e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>Gainesville, FL</td>\n",
       "      <td>Oliver #BLM</td>\n",
       "      <td>obliqueObloquy</td>\n",
       "      <td>492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91269 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coordinates                      created_at  \\\n",
       "0             NaN  Tue Dec 11 01:00:00 +0000 2018   \n",
       "1             NaN  Mon Jan 22 09:49:35 +0000 2018   \n",
       "2             NaN  Mon Sep 17 04:42:16 +0000 2018   \n",
       "3             NaN  Sat Aug 04 13:02:13 +0000 2018   \n",
       "4             NaN  Tue Nov 21 10:17:51 +0000 2017   \n",
       "...           ...                             ...   \n",
       "91264         NaN  Sat Apr 27 16:22:49 +0000 2019   \n",
       "91265         NaN  Sat Apr 27 11:36:03 +0000 2019   \n",
       "91266         NaN  Sat Apr 27 16:23:09 +0000 2019   \n",
       "91267         NaN  Sat Apr 27 16:23:07 +0000 2019   \n",
       "91268         NaN  Sat Apr 27 16:23:32 +0000 2019   \n",
       "\n",
       "                                                hashtags  \\\n",
       "0                               UN cdnpoli ONpoli ABpoli   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3      Spain Portugal climatechange globalwarming Hea...   \n",
       "4      Resist FakePresident Dontard GOP NRA War Clima...   \n",
       "...                                                  ...   \n",
       "91264                                                NaN   \n",
       "91265                     plasticpollution climatechange   \n",
       "91266                                                NaN   \n",
       "91267                                                NaN   \n",
       "91268                                                NaN   \n",
       "\n",
       "                                                   media  \\\n",
       "0      https://twitter.com/TheRebelTV/status/10722948...   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "91264                                                NaN   \n",
       "91265                                                NaN   \n",
       "91266                                                NaN   \n",
       "91267                                                NaN   \n",
       "91268                                                NaN   \n",
       "\n",
       "                                                    urls  favorite_count  \\\n",
       "0      https://www.therebel.media/un-global-warming-m...              91   \n",
       "1                                                    NaN               0   \n",
       "2      https://truthout.org/articles/national-park-of...               0   \n",
       "3      https://news.sky.com/story/live-scorching-satu...               1   \n",
       "4      https://twitter.com/mattmfm/status/93272970237...               0   \n",
       "...                                                  ...             ...   \n",
       "91264                                                NaN               1   \n",
       "91265                                                NaN               0   \n",
       "91266  https://www.usatoday.com/story/news/nation/201...               0   \n",
       "91267                                                NaN               0   \n",
       "91268                                                NaN               0   \n",
       "\n",
       "                        id in_reply_to_screen_name  in_reply_to_status_id  \\\n",
       "0      1072294898588631040                     NaN                    NaN   \n",
       "1       955376892026093569                Pontifex           9.551606e+17   \n",
       "2      1041547863795224576                     NaN                    NaN   \n",
       "3      1025728615399469058                     NaN                    NaN   \n",
       "4       932915956824682496                     NaN                    NaN   \n",
       "...                    ...                     ...                    ...   \n",
       "91264  1122174270208126978           bruceanderson           1.122169e+18   \n",
       "91265  1122102103185547270         RochdaleCouncil                    NaN   \n",
       "91266  1122174355084062720                     NaN                    NaN   \n",
       "91267  1122174343562158081                     NaN                    NaN   \n",
       "91268  1122174449095270403              creeker112           1.122173e+18   \n",
       "\n",
       "       in_reply_to_user_id  ... user_followers_count user_friends_count  \\\n",
       "0                      NaN  ...               207643              17379   \n",
       "1             5.007043e+08  ...                    1                  4   \n",
       "2                      NaN  ...                 2062               2383   \n",
       "3                      NaN  ...                   25                 24   \n",
       "4                      NaN  ...                 6214               6747   \n",
       "...                    ...  ...                  ...                ...   \n",
       "91264         8.037439e+07  ...                  491                779   \n",
       "91265         2.052017e+07  ...                   87                177   \n",
       "91266                  NaN  ...                  110                644   \n",
       "91267                  NaN  ...                 2217               1838   \n",
       "91268         1.108134e+18  ...                   18                327   \n",
       "\n",
       "      user_listed_count              user_location      user_name  \\\n",
       "0                  1266       Canada and the world     Rebel News   \n",
       "1                     0              United States          Frank   \n",
       "2                    99                        USA  OurRevolution   \n",
       "3                     1                        NaN    Steven Hugh   \n",
       "4                   121  the beautiful \"Jemez\" USA      Athoughtz   \n",
       "...                 ...                        ...            ...   \n",
       "91264                 9                     Regina     sean osmar   \n",
       "91265                 6   Wandering the Wastelands      Gatt ðŸ‡¬ðŸ‡§ðŸ‡ªðŸ‡º   \n",
       "91266                 2            Omaha, Nebraska        Dee Dee   \n",
       "91267               450                  Vancouver         Bonnie   \n",
       "91268                 0            Gainesville, FL    Oliver #BLM   \n",
       "\n",
       "       user_screen_name.1 user_statuses_count user_time_zone  \\\n",
       "0         RebelNewsOnline               39345            NaN   \n",
       "1           Frank34802901                 100            NaN   \n",
       "2             LeftysUnite               50076            NaN   \n",
       "3             Steven9Hugh                1402            NaN   \n",
       "4               athoughtz              155954            NaN   \n",
       "...                   ...                 ...            ...   \n",
       "91264           SeanOsmar                2229            NaN   \n",
       "91265               Gatt_                2591            NaN   \n",
       "91266         Dee_Dee2018                2765            NaN   \n",
       "91267      greenurlifenow              254477            NaN   \n",
       "91268      obliqueObloquy                 492            NaN   \n",
       "\n",
       "                               user_urls user_verified  \n",
       "0              https://www.rebelnews.com          True  \n",
       "1                                    NaN         False  \n",
       "2                                    NaN         False  \n",
       "3      https://stevenhugh.wordpress.com/         False  \n",
       "4                      http://TokTok.com         False  \n",
       "...                                  ...           ...  \n",
       "91264                                NaN         False  \n",
       "91265                                NaN         False  \n",
       "91266                                NaN         False  \n",
       "91267                                NaN         False  \n",
       "91268                                NaN         False  \n",
       "\n",
       "[91269 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/tweets_125k.csv\")\n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is many columns. Most of these won't help me so many will have to go. Others need some fixing up to be useful like the date and time string need to be date objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Wrangling </center>\n",
    "The data set has lots of data that is not needed for this analysis. Since we are looking at sentiment over time and other factors related to polticis of a state and events, it is simplest to just drop all non-English tweets.\n",
    "\n",
    "There are lots of extranenous columns that are not relavent to this project so I just dropped them. These included geolocation data that was often missing, user information, and extranenous data about a tweet like time zone and lanague (since I drop all non-English ones to begin). These include things like user specifics like their profile details and other things like the URL of thr tweet or the language since all will be English. \n",
    "\n",
    "I followed this tutorial for help with tasks like lemmatization here and in the next section (EDA/Data Viz) making bags of words and word clouds.\n",
    "https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-English tweets and extraneous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date/time</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>91</td>\n",
       "      <td>55</td>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Mon Sep 17 04:42:02 +0000 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122174270208126978</th>\n",
       "      <td>Sat Apr 27 16:22:49 +0000 2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@bruceanderson What's the issue? He agrees cli...</td>\n",
       "      <td>SeanOsmar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122102103185547270</th>\n",
       "      <td>Sat Apr 27 11:36:03 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@RochdaleCouncil need to start accepting ALL p...</td>\n",
       "      <td>Gatt_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122174355084062720</th>\n",
       "      <td>Sat Apr 27 16:23:09 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Climate change, global warming: Grandparents t...</td>\n",
       "      <td>Dee_Dee2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122174343562158081</th>\n",
       "      <td>Sat Apr 27 16:23:07 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Of course, each person contributes to the stat...</td>\n",
       "      <td>greenurlifenow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122174449095270403</th>\n",
       "      <td>Sat Apr 27 16:23:32 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@creeker112 @IlhanMN Well, capitalism is behin...</td>\n",
       "      <td>obliqueObloquy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83624 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          date/time  favorite_count  \\\n",
       "tweetID                                                               \n",
       "1072294898588631040  Tue Dec 11 01:00:00 +0000 2018              91   \n",
       "955376892026093569   Mon Jan 22 09:49:35 +0000 2018               0   \n",
       "1025728615399469058  Sat Aug 04 13:02:13 +0000 2018               1   \n",
       "932915956824682496   Tue Nov 21 10:17:51 +0000 2017               0   \n",
       "1041547806622797824  Mon Sep 17 04:42:02 +0000 2018               3   \n",
       "...                                             ...             ...   \n",
       "1122174270208126978  Sat Apr 27 16:22:49 +0000 2019               1   \n",
       "1122102103185547270  Sat Apr 27 11:36:03 +0000 2019               0   \n",
       "1122174355084062720  Sat Apr 27 16:23:09 +0000 2019               0   \n",
       "1122174343562158081  Sat Apr 27 16:23:07 +0000 2019               0   \n",
       "1122174449095270403  Sat Apr 27 16:23:32 +0000 2019               0   \n",
       "\n",
       "                     retweet_count  \\\n",
       "tweetID                              \n",
       "1072294898588631040             55   \n",
       "955376892026093569               0   \n",
       "1025728615399469058              0   \n",
       "932915956824682496               0   \n",
       "1041547806622797824              1   \n",
       "...                            ...   \n",
       "1122174270208126978              0   \n",
       "1122102103185547270              0   \n",
       "1122174355084062720              0   \n",
       "1122174343562158081              0   \n",
       "1122174449095270403              0   \n",
       "\n",
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "...                                                                ...   \n",
       "1122174270208126978  @bruceanderson What's the issue? He agrees cli...   \n",
       "1122102103185547270  @RochdaleCouncil need to start accepting ALL p...   \n",
       "1122174355084062720  Climate change, global warming: Grandparents t...   \n",
       "1122174343562158081  Of course, each person contributes to the stat...   \n",
       "1122174449095270403  @creeker112 @IlhanMN Well, capitalism is behin...   \n",
       "\n",
       "                             tweeter  \n",
       "tweetID                               \n",
       "1072294898588631040  RebelNewsOnline  \n",
       "955376892026093569     Frank34802901  \n",
       "1025728615399469058      Steven9Hugh  \n",
       "932915956824682496         athoughtz  \n",
       "1041547806622797824   IndiaGreenBldg  \n",
       "...                              ...  \n",
       "1122174270208126978        SeanOsmar  \n",
       "1122102103185547270            Gatt_  \n",
       "1122174355084062720      Dee_Dee2018  \n",
       "1122174343562158081   greenurlifenow  \n",
       "1122174449095270403   obliqueObloquy  \n",
       "\n",
       "[83624 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-English tweets from the data set\n",
    "data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "# Drop all the unneeded columns from the data set\n",
    "cols_to_delete = [\"user_urls\", \"user_statuses_count\", \"coordinates\", \"user_name\", \"in_reply_to_status_id\", \n",
    "                  \"in_reply_to_user_id\", \"user_time_zone\", \"urls\", \"lang\", \"media\", \"source\", \n",
    "                  \"retweet_screen_name\", \"retweet_id\", \"possibly_sensitive\", \"tweet_url\",\n",
    "                  \"user_default_profile_image\", \"user_friends_count\", \"user_verified\", \"user_location\", \n",
    "                   \"in_reply_to_screen_name\", \"user_screen_name.1\",\n",
    "                  \"user_favourites_count\", \"user_listed_count\", \"user_created_at\", \"user_description\", \"place\", \n",
    "                 \"user_followers_count\", \"hashtags\"]\n",
    "\n",
    "data = data.drop(columns=cols_to_delete)\n",
    "\n",
    "# Swap the index column from 0...n to the tweet ID and rename the column from id to tweetID and rename to clarify\n",
    "# column meaning\n",
    "data = data.rename(columns={\"id\": \"tweetID\", \"created_at\": \"date/time\", \"user_screen_name\": \"tweeter\"})\n",
    "data = data.set_index('tweetID')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the likes and retweets for total interaction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date/time</th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Mon Sep 17 04:42:02 +0000 2018</td>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          date/time  \\\n",
       "tweetID                                               \n",
       "1072294898588631040  Tue Dec 11 01:00:00 +0000 2018   \n",
       "955376892026093569   Mon Jan 22 09:49:35 +0000 2018   \n",
       "1025728615399469058  Sat Aug 04 13:02:13 +0000 2018   \n",
       "932915956824682496   Tue Nov 21 10:17:51 +0000 2017   \n",
       "1041547806622797824  Mon Sep 17 04:42:02 +0000 2018   \n",
       "\n",
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions  \n",
       "tweetID                                                   \n",
       "1072294898588631040  RebelNewsOnline                 146  \n",
       "955376892026093569     Frank34802901                   0  \n",
       "1025728615399469058      Steven9Hugh                   1  \n",
       "932915956824682496         athoughtz                   0  \n",
       "1041547806622797824   IndiaGreenBldg                   4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the number of favorites and retweets for a tweet into an total interactions score\n",
    "total_interactions = []\n",
    "\n",
    "for row in data.iterrows():\n",
    "    tweet = row[1] \n",
    "    total = tweet[\"retweet_count\"] + tweet[\"favorite_count\"]\n",
    "    total_interactions.append(total)\n",
    "\n",
    "# Swap out the current RT and favorites columns for the total interactions columns\n",
    "data[\"total_interactions\"] = total_interactions\n",
    "data = data.drop(columns=[\"retweet_count\", \"favorite_count\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the time/date strings into date objects using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \n",
       "tweetID                                                                \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11  \n",
       "955376892026093569     Frank34802901                   0   2018-01-22  \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04  \n",
       "932915956824682496         athoughtz                   0   2017-11-21  \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dates time strings into datetime objects\n",
    "dates = []\n",
    "\n",
    "# Matching this text\n",
    "# Mon Jan 22 09:49:35 +0000 2018\n",
    "# For every row in the dataframe\n",
    "regex = re.compile(r\"(\\w{3}) (\\w{3}) (\\d\\d) (\\d\\d:\\d\\d:\\d\\d) \\+(0{4}) (\\d{4})\")\n",
    "\n",
    "# Given a string of a month return the corresponding integer for that month i.e. Jan == 1\n",
    "def numerize(str):\n",
    "    month = str.lower()\n",
    "    if (month == \"jan\"): return 1\n",
    "    elif (month == \"feb\"): return 2\n",
    "    elif (month == \"mar\"): return 3\n",
    "    elif (month == \"apr\"): return 4\n",
    "    elif (month == \"may\"): return 5\n",
    "    elif (month == \"jun\"): return 6\n",
    "    elif (month == \"jul\"): return 7 \n",
    "    elif (month == \"aug\"): return 8\n",
    "    elif (month == \"sep\"): return 9\n",
    "    elif (month == \"oct\"): return 10\n",
    "    elif (month == \"nov\"): return 11\n",
    "    elif (month == \"dec\"): return 12\n",
    "        \n",
    "for row in data.iterrows():\n",
    "    dt = row[1][\"date/time\"]\n",
    "    matches = re.search(regex, dt)\n",
    "    groups = matches.groups()    \n",
    "    month = numerize(groups[1])\n",
    "    d = datetime.date(int(groups[5]), month, int(groups[2]))\n",
    "    dates.append(d)\n",
    "    \n",
    "data = data.drop(columns=[\"date/time\"])\n",
    "data[\"date_tweeted\"] = dates\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tweets for sentiment analysis\n",
    "There are several things that need to be done to the actual text of the tweets before we can do sentiment analysis on them. To starts of, I will do some basic things like make all tweet bodies lower case so that words like CLIMATE and climate and cLiMate are all treated the same by the model I use later on. Next, I am going to remove all links from these tweets because that is irrelvanet to the sentiment of the tweet. There are many things like this that I will do here then I will move on to make the tweets \"linguistically sound\" for analysis by doing things like removing words without meaning that won't contribute the analysis and then making all words their base word or lemmatizing them.\n",
    "\n",
    "### Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \n",
       "tweetID                                                                \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11  \n",
       "955376892026093569     Frank34802901                   0   2018-01-22  \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04  \n",
       "932915956824682496         athoughtz                   0   2017-11-21  \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkless = []\n",
    "regex = re.compile(r\"http\\S+\")\n",
    "\n",
    "# remove all links from each tweet\n",
    "for row in data.iterrows():\n",
    "    txt = row[1][\"text\"]\n",
    "    if txt.find(\"https://t.co\"): \n",
    "        ll = re.sub(regex, \"\", txt)\n",
    "        linkless.append(ll)\n",
    "    else: \n",
    "        linkless.append(txt)\n",
    "\n",
    "data[\"text\"] = linkless\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Preprocess\" the tweets\n",
    "I will be using GenSim to do much of the natural langauge processing in this project. \"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\"\n",
    "#### Tokenization\n",
    "Tokenization a fundemental first step before you do anything in code with natural language to understand it. Tokenization is taking a piece of text like \"Python is the best\" and making into a list of the words that make up that text so here that would be [\"Python\", \"is\", \"the\", \"best\"]. You can find a more in-depth explanation of tokenization here if you're so interested: https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/. Though I never explicity call something like tokenize(tweet), the preprocess function right here and the lemmatize later on are doing this. The reasons I do this twice is I want to clean up the tweet before I lemmatize (explained later) and lemmatize needs a string not a string list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[therebeltv, goes, to, two, different, un, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>[pontifex, prayers, to, god, the, one, amp, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[red, alert, in, spain, and, portugal, as, eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[trump, gop, are, the, swamp, resist, fakepres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[study, green, buildings, save, billion, in, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                                 \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11   \n",
       "955376892026093569     Frank34802901                   0   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04   \n",
       "932915956824682496         athoughtz                   0   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17   \n",
       "\n",
       "                                                                tokens  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  [therebeltv, goes, to, two, different, un, con...  \n",
       "955376892026093569   [pontifex, prayers, to, god, the, one, amp, on...  \n",
       "1025728615399469058  [red, alert, in, spain, and, portugal, as, eur...  \n",
       "932915956824682496   [trump, gop, are, the, swamp, resist, fakepres...  \n",
       "1041547806622797824  [study, green, buildings, save, billion, in, h...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for r in data.iterrows():\n",
    "    tweets.append(gensim.utils.simple_preprocess(r[1][\"text\"]))\n",
    "\n",
    "# don't need the text columns anymore\n",
    "data = data.drop(columns=[\"text\"])\n",
    "\n",
    "data[\"tokens\"] = tweets \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and Lemmatization \n",
    "Here I will remove stopwards from the tweet bodies. These are words like \"I\" and \"this\" that add little meaning to the tweet but if left in the text will give me an innacurate depiction of the most common words in the tweets. Thne I will perform lemmatization on the tweets. This just means taking words that linguisticlly mean the same thing like walker and walking and reducing them to their base. In this case the word walk. You can read more here: https://en.wikipedia.org/wiki/Lemmatisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[therebeltv, go, different, un, conference, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>[pontifex, prayer, god, amp, true, father, jes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[red, alert, spain, portugal, europe, near, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[trump, gop, swamp, resist, fakepresident, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[study, green, building, save, billion, health...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                                 \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11   \n",
       "955376892026093569     Frank34802901                   0   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04   \n",
       "932915956824682496         athoughtz                   0   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17   \n",
       "\n",
       "                                                                tokens  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  [therebeltv, go, different, un, conference, me...  \n",
       "955376892026093569   [pontifex, prayer, god, amp, true, father, jes...  \n",
       "1025728615399469058  [red, alert, spain, portugal, europe, near, ti...  \n",
       "932915956824682496   [trump, gop, swamp, resist, fakepresident, don...  \n",
       "1041547806622797824  [study, green, building, save, billion, health...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model \n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "lemmatized = []\n",
    "# Lemmatization of tweets and stopwords removal\n",
    "for r in data.iterrows():\n",
    "    tweet = r[1][\"tokens\"] # get tokens\n",
    "    text = \" \".join(tweet) # need as string to lemmatize\n",
    "    lemmas = [tok.lemma_ for tok in list(model(text)) if (tok.is_stop==False)] # lematize\n",
    "    lemmatized.append(lemmas) # save lemma tokens\n",
    "    \n",
    "data[\"tokens\"] = lemmatized\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Exploratory Data Analysis and Data Visualization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will look at a number of aspects of the data. I will start off by just looking at when do people tweet about climate the most. Then I will do a number of word cloud visuazlaitons by things like year and peak times for tweeting about climate. To do these, I create bags of words and then sum over the rows so I make the word cloud using a one row data frame. Then I will look at the \"length\" of the tweet (normalzied for stopwords and semantically useless words). Finally, I will visualize a distribution of who is tweeting about climate as in how much are different people contributing to the data.\n",
    "\n",
    "In addition to the I mentioned above (https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/), I also referenced here to make word cloud visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Frequency Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a year column to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make splitting up the data frame up by time easier, I will add a year column to the tweet\n",
    "years = []\n",
    "for r in data.iterrows(): \n",
    "    years.append(r[1][\"date_tweeted\"].year)\n",
    "data[\"year\"] = years\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tweets per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there times where people tweet about climate change more than others?\n",
    "# To make things simple lets look at the number of tweets per month year pair in the dataset\n",
    "tweets_per_month = {}\n",
    "# As you may recall data was collected from September 21, 2017 and May 17, 2019 and they had a gap in \n",
    "# data collection from January 7, 2019 to April 17, 2019.\n",
    "\n",
    "# Initalize the value for a tuple key for the month/year pair to 0 for all months where data was collected\n",
    "# 2017\n",
    "for i in [9, 10, 11, 12]: \n",
    "    tweets_per_month[(i,2017)] = 0\n",
    "# 2018\n",
    "for i in range(1,13): \n",
    "    tweets_per_month[(i,2018)] = 0\n",
    "# 2019\n",
    "for i in [1, 4, 5]: \n",
    "    tweets_per_month[(i,2019)] = 0\n",
    "    \n",
    "# Iterate over data frame and add one to each tweet's proper month/year tuple's value\n",
    "for r in data.iterrows():\n",
    "    row = r[1]\n",
    "    date = row[\"date_tweeted\"]\n",
    "    tweets_per_month[(date.month, date.year)] += 1\n",
    "    \n",
    "# Remove Septembr 2017 and January-April 2019 because most during most the month data was not collected\n",
    "tweets_per_month.pop((9, 2017))\n",
    "tweets_per_month.pop((1, 2019))\n",
    "tweets_per_month.pop((4, 2019))\n",
    "# I decided to also drop May 2019 from this visualization because if I leave it then there is a 5 month gap in\n",
    "# data and this is not reflected in the plot.\n",
    "tweets_per_month.pop((5, 2019))\n",
    "\n",
    "# Convert the tuples to strings\n",
    "time = list(tweets_per_month.keys())\n",
    "count = list(tweets_per_month.values())\n",
    "\n",
    "dates = []\n",
    "for t in time: \n",
    "    dates.append(str(t[0]) + \"/\" + str(t[1])[-2:])\n",
    "    \n",
    "print(\"Tweets per month:\")\n",
    "tweets_per_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot frequency of tweeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['date', 'tweet_count'])\n",
    "df['date'] = dates\n",
    "df['tweet_count'] = count\n",
    "\n",
    "dims = (9, 5)\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "plot = sns.lineplot(ax=ax, data=df, x=df['date'], y=df['tweet_count'])\n",
    "plot = plot.set(title=\"Tweet Frequency (October 2017 to December 2018)\",xlabel=\"Time\",ylabel=\"Number of Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there were several time periods where the frequency of tweeting about climate was a bit higher than others, August and December 2018. I will look into these time periods a bit more below. Though, we can see that for the most part, people tweet a good amount about climate throughout the year and there really isn't one obvious patter that we can learn from this graph other than maybe people tweet alot about climate in the spring but again we are only looking at limited data so that inference is hard to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and Word Clouds\n",
    "Here I am going to write be converting sets of tweets into bags of words. What that basically means is I look at a set of tweets and then will make a matrix representation of these tweets. So every possible word in all the tweets will be the columns and the tweet ids will be my rows. Each row is a tweet. Every word in all the tweets gets a column and if tweet 10 in its row has \"java\" 10 times then it will have a ten in that cell, if it has \"cowboy\" 0 times than it has a 0 there. Most cells will be 0 because there are LOTS of columns but only so many words one uses in any single tweet.\n",
    "\n",
    "I will then use these bags of words to make word clouds which will represent the bag of words with words on an image where the more a word is used, the bigger it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were the terms used for collecting tweets, terms with space have been made into two elements and duplicates\n",
    "# are remoevd. The final one amp, is just HTML for \"&\" and so we do not want that in the visuals because its not a\n",
    "# word and its not semantically meaningful\n",
    "rm = [\"climatechange\", \"climate\", \"change\", \"real\", \"act\", \"climate\", \"global\", \"warming\", \"hoax\", \"denier\", \n",
    "      \"false\", \"not\", \"amp\"]\n",
    "\n",
    "# Object to vectorize words into a bag of words?\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "\n",
    "# add a column with the text of each tweet as one string\n",
    "tweets = []\n",
    "for r in data.iterrows():\n",
    "    tweets.append(\" \".join(r[1][\"tokens\"]))\n",
    "data[\"tweet\"] = tweets\n",
    "\n",
    "# define a function that takes in a data frame of tweets and returns a bag of words w/ the help of sklearn\n",
    "def make_bag_of_words(tweets):\n",
    "    info = cv.fit_transform(tweets[\"tweet\"])\n",
    "    bow = pd.DataFrame(info.toarray(), columns=cv.get_feature_names())\n",
    "    bow.drop(columns=rm, inplace=True)\n",
    "    bow.index=tweets.index\n",
    "    return bow\n",
    "\n",
    "# make a bag of words for each year i have data on\n",
    "bags = []\n",
    "for i in range(2017,2020): \n",
    "    bags.append(make_bag_of_words(data[data[\"year\"] == i]))\n",
    "    \n",
    "# example output for 2018\n",
    "bags[2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shockingly, you probably aren't getting much insight from looking at this massisve matrix of mostly 0s. So lets create a function to take that data and make it into a word cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function that makes a word cloud, given a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a function that will display a word cloud give the word frequencies as a df\n",
    "def make_word_cloud(freqs):\n",
    "    wc = WordCloud(width=1000, height=800, max_words=50,colormap=\"Dark2\").generate_from_frequencies(freqs.sum())\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call make word cloud each bag of words summed so its one row for all tweets in that year (2017-19)\n",
    "title = [\"2017\", \"2018\", \"2019\"]\n",
    "for i in range(3): \n",
    "    print(title[i]+ \" Climate Change Tweets\")\n",
    "    make_word_cloud(bags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, it does not look like the words people use to tweet about climate are all that different year over year. Its lots of \"climate change\", \"#climatechagne\", \"global warming\", \"world\", and different strings refering to President Trump. But do keep in mind the data from 2018 dominates that data set and we only have data from the start of 2019 and end of 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most interacted with tweets\n",
    "Here I will use the WordCloud library to visualize the 1000 most interacted with tweets. I define an interaction as a favorite or retweet and I weighted them equally when I combined the two earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a word count of the most interacted with tweets (what got the most likes and retweets)\n",
    "data = data.sort_values(by=[\"total_interactions\"], ascending=False)\n",
    "top_1000 = data.head(1000)\n",
    "print(\"Top 1000 Most Interacted With Tweets\")\n",
    "make_word_cloud(make_bag_of_words(top_1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the popular kids on Twitter are largely tweeting the same words as everyone else. Could be interesting later on to see if they have a different sentiment though?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During the peak times for climate on Twitter, what were people tweeting about? Was it a natural distaster, a political move, something else, or nothing special?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twentyseventeen = data[data[\"year\"] == 2017] \n",
    "start = datetime.date(2017, 12, 1) # start of december\n",
    "dec_2017 = twentyseventeen[twentyseventeen[\"date_tweeted\"] >= start] \n",
    "bow_12_17 = make_bag_of_words(dec_2017)\n",
    "print(\"December 2017 Climate Change Tweets\")\n",
    "make_word_cloud(bow_12_17) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Length\" of Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_length']=data['tokens'].apply(lambda tokens: len(tokens))\n",
    "plot = sns.barplot(x='year',y='tweet_length',data=data)\n",
    "plot.set(title=\"Normalized Length of Tweet by Year\",xlabel=\"Year\",ylabel=\"Length\")\n",
    "\n",
    "# go ahead the drop the tweet length so df is more readable/short width later on\n",
    "data = data.drop(columns=[\"tweet_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the length of the Tweet that we are looking at here is approximate because we removed things like links (which are notably not words) and GenSim's preprocess() function removes very short and long words from the tweets to help me get the most semantically meanigful words from the tweets.\n",
    "\n",
    "When looking at this graph one major thing to consider is that it may not be such a simple oh tweet length about climate must be going up year after year for a couple of reasons. One major factor is the sample from 2018 is much larger than the other two because the majority of data colleciton took place in 2018 and much of Jan-Apr 2019 lacked sampling. In 2019, GWU collected data the first week of January, not at all in Feb-Mar, the last two weeks of April, and most of May.\n",
    "\n",
    "So roughly speaking we can say that tweet length may have increased when talking about tweets related to climate change but because of what I just said this is at best a rough guess because we aren't really saying massive changes in length for example between 2018 and 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are we just seeing a few dominant voices on Twitter?\n",
    "Lets look at the distribution of tweet count about climate to see if these most of these tweets are coming from a few tweeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of tweeters \n",
    "tweeters = list(set(data[\"tweeter\"]))\n",
    "times_tweeted = dict.fromkeys(tweeters , 0)\n",
    "\n",
    "# Calculate how many times each tweeter has tweeted\n",
    "for r in data.iterrows():\n",
    "    row = r[1]\n",
    "    user = row[\"tweeter\"]\n",
    "    times_tweeted[user] += 1\n",
    "    \n",
    "# get key and values as lists\n",
    "users = list(times_tweeted.keys())\n",
    "counts = list(times_tweeted.values())\n",
    "\n",
    "# put them in a dataframe\n",
    "df = pd.DataFrame(columns = ['user', 'tweet count'])\n",
    "df[\"user\"] = users\n",
    "df[\"tweet count\"] = counts\n",
    "\n",
    "# plot the distribution using displot in sns\n",
    "plot = sns.displot(df, log_scale=True)\n",
    "plot.set(title=\"Distribution of Who Is Doing The Tweeting\", ylabel=\"Number of Tweets\", xlabel=\"Number of Users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly every person in the data set has tweeted between 1-10 times about climate. This plot is very skewed (right). So we are NOT just seeing a few voiced but many voices on climate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Sentiment Analysis and Machine Learning</center>\n",
    "For my section on machine learning I going to primarily focus on sentiment analysis. I will give a brief introduction to what that is but if you are interesting in learning more, I would reccommend that you check out these links that dive deeper and may explain it a bit better: https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17, https://monkeylearn.com/sentiment-analysis/.\n",
    "\n",
    "So what is sentiment analysis? On a basic level its looking at the words used in a piece of text like \"Climate change is my favorite thing to tweet about ðŸ˜.\" and seeing if the text is neutral, positive, or negative. This example would likely be said to have positive sentiment based on the word favorite and the emoji if that was considered in the calculation.\n",
    "\n",
    "Something to conisder here: [\"U.S. adult Twitter users are younger and more likely to be Democrats than the general public\"](https://www.pewresearch.org/internet/2019/04/24/sizing-up-twitter-users/). I do realize that every young Democrat cares about the planet but we need to consider this when looking at sentiment. This is sort of obvious later on when I looked at the number of positive vs number of negative tweets about climate that I can label based on the words used (directly below) because there are SO MANY more tweets that are positive that I am able to label about climate than the negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign each tweet an overall sentiment score \n",
    "Here I am using VADER and its compound sentiment analysis. The compound score combines the fraction of its neutral, positive, and negative sentiment into one sentiment score. The compound score will be between -1 (very negative) and +1 (very positive). You can read more about this in the read me of their repository on GitHub [here](https://github.com/cjhutto/vaderSentiment#about-the-scoring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found vader from this article\n",
    "# https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair\n",
    "# It's optimized for social network data and gets good results when using that sort of data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_values = []\n",
    "\n",
    "for r in data.iterrows():\n",
    "    tweet = r[1][\"tweet\"]\n",
    "    # returns the degree to which its positive/negative/netural and a compound score combining all of those\n",
    "    scores = analyzer.polarity_scores(tweet)\n",
    "    # store the compound score\n",
    "    compound = scores[\"compound\"]\n",
    "    sentiment_values.append(compound)\n",
    "\n",
    "data[\"sentiment_scores\"] = sentiment_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each tweet as positive, negative, or neutral in sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the guidlines for deciding if a tweet is positive/negative/neutral layed out by VADER here:\n",
    "# https://github.com/cjhutto/vaderSentiment#about-the-scoring\n",
    "sentiment_label = []\n",
    "\n",
    "POSITIVE = 1\n",
    "NEGATIVE = -1\n",
    "NEUTRAL = 0\n",
    "\n",
    "for val in sentiment_values:\n",
    "    #positive sentiment: compound score >= 0.05\n",
    "    if val >= 0.05:\n",
    "        sentiment_label.append(POSITIVE)\n",
    "    #neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    elif (val > -0.05) and (val < .05):\n",
    "        sentiment_label.append(NEUTRAL)\n",
    "    #negative sentiment: compound score <= -0.05\n",
    "    elif val <= -0.05:\n",
    "        sentiment_label.append(NEGATIVE)\n",
    "\n",
    "data[\"sentiment_label\"] = sentiment_label\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Distribution Across The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to pie graph the sentiment of the tweets in a data frame, takes in a df and a graph title\n",
    "def graph_sentiment(df, title): \n",
    "    labels = 'POSITIVE', 'NEGATIVE', 'NEUTRAL'\n",
    "    sizes = [len(df[df[\"sentiment_label\"] == POSITIVE].index), len(df[df[\"sentiment_label\"] == NEGATIVE].index), \n",
    "         len(df[df[\"sentiment_label\"] == NEUTRAL].index)]\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels,autopct='%1.1f%%',shadow=True)\n",
    "    plt.title(label=title)\n",
    "    ax1.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot all tweets' sentiment\n",
    "graph_sentiment(data,\"Sentiments of Tweets 2017-2019\")\n",
    "\n",
    "# Lets look at sentiment of the top 1000 tweets compared to the overall\n",
    "top_1000 = data.sort_values(by=[\"total_interactions\"], ascending=False).head(1000)\n",
    "graph_sentiment(top_1000,\"Sentiments of Top 1,000 Tweets 2017-2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a couple of things are goin on here. Looking at all the data, most tweets are positive in sentiment towards climate change but its no majority. Also, a fair amount of tweets are labeled as neutral but its about half as many compared with the positive sentiment tweets.\n",
    "\n",
    "In regards to the bottom graph that looked at the top 1,000 most interacted with tweets (i.e. sum of retweets and favorites), they were a good bit more negative that the full data set by about 5% and they were less neutral. Though, the positive percentage is basically the same for all tweets and the most interacted with ones. One could may make an assumption that more polarzing or negative tweets are more negative but I am not sure we have the full evidence for that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "I will be following this tutorial on how to work with text data and do ML with it: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform documents into feature vectors\n",
    "Explain this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the documents to feature vectors\n",
    "count_vect = CountVectorizer()\n",
    "# Train with 80% of the data\n",
    "training_data = data.sample(frac=8/10) \n",
    "\n",
    "# Learns vocab used in training tweets, returns document term matrix\n",
    "# same as fitting then transforming\n",
    "# but slightly more efficent\n",
    "X_train_counts = count_vect.fit_transform(training_data[\"tweet\"])\n",
    "\n",
    "# Test it out with a common word in the data set\n",
    "print(\"The word climate is used\", count_vect.vocabulary_.get(u'climate'), \"times in all the training tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit estimator to data\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True).fit(X_train_counts) \n",
    "# transform the count matrix into a tf-idf representation\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts) \n",
    "print(\"The shape of the the TF-IDF matrix is\",X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "clf = MultinomialNB().fit(X_train_tfidf, training_data[\"sentiment_label\"])\n",
    "\n",
    "# Build a df of test data that is not in training data\n",
    "df = data.copy().drop(training_data.index)\n",
    "X_test= df[\"tweet\"]\n",
    "\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "# function to Print the first 10 predicted sentiments with the corresponding tweet\n",
    "def print_predictions(X_test, predicted):\n",
    "    for (tweet, sentiment) in zip(X_test[:10], predicted[:10]):\n",
    "        result = \"-\"\n",
    "        # matche the sentiment integer to its meaning\n",
    "        if sentiment == POSITIVE:\n",
    "            result += \"Positive: \" \n",
    "        elif sentiment == NEGATIVE:\n",
    "            result += \"Negative: \" \n",
    "\n",
    "        elif sentiment == NEUTRAL:\n",
    "            result += \"Neutral: \"\n",
    "        result += tweet + \"\\n\"\n",
    "        print(result)\n",
    "        \n",
    "print_predictions(X_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn's Pipline class to classify tweets with a random forest\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', RandomForestClassifier()),\n",
    " ])\n",
    "text_clf.fit(training_data[\"tweet\"], training_data[\"sentiment_label\"])\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print_predictions(X_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"tweeter\"] == \"AOC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Insight </center>\n",
    "So what did we learn here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Acknowledgements </center>\n",
    "I would like to thank the George Washington University's TweetSets organization for their data set on climate change on Twitter. Their data and website to sample the data set made this project so much easier. Once again all of their data sets from Twitter can be found right [here](https://tweetsets.library.gwu.edu).\n",
    "\n",
    "I would also like to thank the UMD CS Prof. John Dickerson and his CMSC320 instructional staff for their help and advice while working on this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
