{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tide of Climate on Twitter: Twitter Climate Change Sentiment Analysis  \n",
    "## By Arjun Gandhi\n",
    "#### Last updated on December 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection\n",
    "I am starting off with a data set from Harvard that contains 39.6 million tweets related to climate change. The data set is in tweet IDs (numbers) so I need get the tweets for each tweet ID.\n",
    "\n",
    "Here is the link the data set: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU\n",
    "\n",
    "As states in the above link the data is from September 21, 2017 and May 17, 2019 and they had a gap in data collection from January 7, 2019 to April 17, 2019.\n",
    "\n",
    "To convert each tweet ID into the actual tweet data I am using this: Hydrator [Computer Software]. Retrieved from https://github.com/docnow/hydrator\n",
    "\n",
    "From the above repo, I downloaded this version of the app: https://github.com/DocNow/hydrator/releases/tag/v0.0.13\n",
    "\n",
    "The tweets are seperated by file (~ 10 million tweets/file). I made a Twitter account to connect my account this Hydrator. I then uploaded each txt file into Hydrator under \"Datasets\" in the desktop app. \n",
    "\n",
    "TALK ABOUT THEIR METHODOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./data/tweets200k.csv\")\n",
    "data = data.head(5000) # REMOVE THIS LINE !!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "The data set has lots of data that is not needed for this analysis. Since we are looking at sentiment over time and other factors related to polticis of a state and events, it is simplest to just drop all non-English tweets.\n",
    "\n",
    "There are lots of extranenous columns that are not relavent to this project so I just dropped them. These include things like user specifics like their profile details and other things like the URL of thr tweet or the language since all will be English. \n",
    "\n",
    "I then renamed some columns for my ease of use of the data set and switched the tweet ID to be the index columns.\n",
    "\n",
    "The date and time is given as a string so I use regular expressions to convert that to a date time object. The hashtag column is given as one string so I split that into a list of hastags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English tweets from the data set\n",
    "data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "# Drop all the unneeded columns from the data set\n",
    "cols_to_delete = [\"user_urls\", \"user_statuses_count\", \"coordinates\", \"user_name\", \"in_reply_to_status_id\", \n",
    "                  \"in_reply_to_user_id\", \"user_time_zone\", \"urls\", \"lang\", \"media\", \"source\", \n",
    "                  \"retweet_screen_name\", \"retweet_id\", \"possibly_sensitive\", \"tweet_url\",\n",
    "                  \"user_default_profile_image\", \"user_friends_count\", \"user_verified\", \"user_location\", \n",
    "                   \"in_reply_to_screen_name\", \"user_screen_name.1\",\n",
    "                  \"user_favourites_count\", \"user_listed_count\", \"user_created_at\", \"user_description\", \"place\", \n",
    "                 \"user_followers_count\"]\n",
    "\n",
    "data = data.drop(columns=cols_to_delete)\n",
    "\n",
    "# Swap the index column from 0...n to the tweet ID and rename the column from id to tweetID and rename to clarify\n",
    "# column meaning\n",
    "data = data.rename(columns={\"id\": \"tweetID\", \"created_at\": \"date/time\", \"user_screen_name\": \"tweeter\"})\n",
    "data = data.set_index('tweetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dates time strings into datetime objects\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "dates = []\n",
    "\n",
    "# Matching this text\n",
    "# Mon Jan 22 09:49:35 +0000 2018\n",
    "# For every row in the dataframe\n",
    "regex = re.compile(r\"(\\w{3}) (\\w{3}) (\\d\\d) (\\d\\d:\\d\\d:\\d\\d) \\+(0{4}) (\\d{4})\")\n",
    "\n",
    "# Given a string of a month return the corresponding integer for that month i.e. Jan == 1\n",
    "def numerize(str):\n",
    "    month = str.lower()\n",
    "    if (month == \"jan\"): return 1\n",
    "    elif (month == \"feb\"): return 2\n",
    "    elif (month == \"mar\"): return 3\n",
    "    elif (month == \"apr\"): return 4\n",
    "    elif (month == \"may\"): return 5\n",
    "    elif (month == \"jun\"): return 6\n",
    "    elif (month == \"jul\"): return 7 \n",
    "    elif (month == \"aug\"): return 8\n",
    "    elif (month == \"sep\"): return 9\n",
    "    elif (month == \"oct\"): return 10\n",
    "    elif (month == \"nov\"): return 11\n",
    "    elif (month == \"dec\"): return 12\n",
    "        \n",
    "for row in data.iterrows():\n",
    "    dt = row[1][\"date/time\"]\n",
    "    matches = re.search(regex, dt)\n",
    "    groups = matches.groups()    \n",
    "    month = numerize(groups[1])\n",
    "    d = datetime.date(int(groups[5]), month, int(groups[2]))\n",
    "    dates.append(d)\n",
    "    \n",
    "data = data.drop(columns=[\"date/time\"])\n",
    "data[\"date_tweeted\"] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make hashtags into a list of strings for each tweet\n",
    "# nan is of type float and the rest are strings\n",
    "tags_lsts = []\n",
    "\n",
    "# For every row if its not nan then split the string into a list of strings\n",
    "# if its nan just add nan to the list of lists\n",
    "for row in data.iterrows():\n",
    "    tags = row[1][\"hashtags\"]\n",
    "    if type(tags) is str:\n",
    "        lst = tags.split()\n",
    "        # make all hashtags lowercase to make them easier to compare\n",
    "        lst = list(map(lambda x : x.lower(), lst))\n",
    "        tags_lsts.append(lst)\n",
    "    else:\n",
    "        tags_lsts.append(float(\"nan\"))\n",
    "        \n",
    "# swap out current hashtags column for this list of lists, now df has a column of lists where each row has \n",
    "# the list of all hashtags in the tweet\n",
    "data = data.drop(columns=[\"hashtags\"])\n",
    "data[\"hashatg_list\"] = tags_lsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>hashatg_list</th>\n",
       "      <th>total_interactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[un, cdnpoli, onpoli, abpoli]</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[spain, portugal, climatechange, globalwarming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[resist, fakepresident, dontard, gop, nra, war...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[health, climate, greenbuilding, sustainabilit...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter date_tweeted  \\\n",
       "tweetID                                             \n",
       "1072294898588631040  RebelNewsOnline   2018-12-11   \n",
       "955376892026093569     Frank34802901   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh   2018-08-04   \n",
       "932915956824682496         athoughtz   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg   2018-09-17   \n",
       "\n",
       "                                                          hashatg_list  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040                      [un, cdnpoli, onpoli, abpoli]   \n",
       "955376892026093569                                                 NaN   \n",
       "1025728615399469058  [spain, portugal, climatechange, globalwarming...   \n",
       "932915956824682496   [resist, fakepresident, dontard, gop, nra, war...   \n",
       "1041547806622797824  [health, climate, greenbuilding, sustainabilit...   \n",
       "\n",
       "                     total_interactions  \n",
       "tweetID                                  \n",
       "1072294898588631040                 147  \n",
       "955376892026093569                    0  \n",
       "1025728615399469058                   1  \n",
       "932915956824682496                    0  \n",
       "1041547806622797824                   4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the number of favorites and retweets for a tweet into an total interactions score\n",
    "total_interactions = []\n",
    "\n",
    "for row in data.iterrows():\n",
    "    tweet = row[1] \n",
    "    total = tweet[\"retweet_count\"] + tweet[\"favorite_count\"]\n",
    "    total_interactions.append(total)\n",
    "\n",
    "# Swap out the current RT and favorites columns for the total interactions columns\n",
    "data[\"total_interactions\"] = total_interactions\n",
    "data = data.drop(columns=[\"retweet_count\", \"favorite_count\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tweets for sentiment analysis\n",
    "There are several things that need to be done to the actual text of the tweets before we can do sentiment analysis on them. To starts of, I will do some basic things like make all tweet bodies lower case so that words like CLIMATE and climate and cLiMate are all treated the same by the model I use later on. Next, I am going to remove all links from these tweets because that is irrelvanet to the sentiment of the tweet. \n",
    "\n",
    "Then I will start do some more linguistic/NLP things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all tweets lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all tweets lower case so that when use a model to look at sentiment words that are the same are treated\n",
    "# so by the model i.e. capitalization won't make the model think Word is not word.\n",
    "\n",
    "lower_case_tweets = []\n",
    "for r in data.iterrows(): lower_case_tweets.append(r[1][\"text\"].lower())\n",
    "data = data.drop(columns=[\"text\"])\n",
    "data[\"text\"] = lower_case_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove links from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweeter</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>hashatg_list</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[un, cdnpoli, onpoli, abpoli]</td>\n",
       "      <td>147</td>\n",
       "      <td>.@therebeltv goes to two different #un confere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@pontifex prayers  to god the one &amp;amp; only t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[spain, portugal, climatechange, globalwarming...</td>\n",
       "      <td>1</td>\n",
       "      <td>red alert in #spain and #portugal as europe ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>athoughtz</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[resist, fakepresident, dontard, gop, nra, war...</td>\n",
       "      <td>0</td>\n",
       "      <td>trump /gop are the swamp #resist #fakepresiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[health, climate, greenbuilding, sustainabilit...</td>\n",
       "      <td>4</td>\n",
       "      <td>study: green buildings save $6.7 billion in #h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tweeter date_tweeted  \\\n",
       "tweetID                                             \n",
       "1072294898588631040  RebelNewsOnline   2018-12-11   \n",
       "955376892026093569     Frank34802901   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh   2018-08-04   \n",
       "932915956824682496         athoughtz   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg   2018-09-17   \n",
       "\n",
       "                                                          hashatg_list  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040                      [un, cdnpoli, onpoli, abpoli]   \n",
       "955376892026093569                                                 NaN   \n",
       "1025728615399469058  [spain, portugal, climatechange, globalwarming...   \n",
       "932915956824682496   [resist, fakepresident, dontard, gop, nra, war...   \n",
       "1041547806622797824  [health, climate, greenbuilding, sustainabilit...   \n",
       "\n",
       "                     total_interactions  \\\n",
       "tweetID                                   \n",
       "1072294898588631040                 147   \n",
       "955376892026093569                    0   \n",
       "1025728615399469058                   1   \n",
       "932915956824682496                    0   \n",
       "1041547806622797824                   4   \n",
       "\n",
       "                                                                  text  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  .@therebeltv goes to two different #un confere...  \n",
       "955376892026093569   @pontifex prayers  to god the one &amp; only t...  \n",
       "1025728615399469058  red alert in #spain and #portugal as europe ne...  \n",
       "932915956824682496   trump /gop are the swamp #resist #fakepresiden...  \n",
       "1041547806622797824  study: green buildings save $6.7 billion in #h...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove links from tweets because they are not helpful in analyzing sentiment\n",
    "\n",
    "# I found this regex here: https://regexr.com/3e6m0\n",
    "linkless = []\n",
    "regex = re.compile(r\"http\\S+\")\n",
    "\n",
    "# remove all links from each tweet\n",
    "for row in data.iterrows():\n",
    "    txt = row[1][\"text\"]\n",
    "    if txt.find(\"https://t.co\"): \n",
    "        ll = re.sub(regex, \"\", txt)\n",
    "        linkless.append(ll)\n",
    "    else: \n",
    "        linkless.append(txt)\n",
    "\n",
    "data[\"text\"] = linkless\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and Lemmatization \n",
    "Here I will remove stopwards from the tweet bodies. These are words like \"I\" and \"this\" that add little meaning to the tweet but if left in the text will give me an innacurate depiction of the most common words in the tweets. Thne I will perform lemmatization on the tweets. This just means taking words that linguisticlly mean the same thing like walker and walking and reducing them to their base. In this case the word walk. You can read more here: https://en.wikipedia.org/wiki/Lemmatisation. I will be doing this uisng spaCy (https://spacy.io).\n",
    "\n",
    "You can find installation instructions for spaCy here: https://spacy.io/usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I did these commands to setup spaCy.\n",
    "# pip install -U spacy\n",
    "# pip install -U spacy-lookups-data\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
