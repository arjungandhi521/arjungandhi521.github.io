{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter's Climate Tide\n",
    "## An Analysis of Tweets About Climate Change \n",
    "### By Arjun Gandhi\n",
    "### Last updated: December 13, 2020\n",
    "### CURRENTLY THIS IS A DRAFT, NOT THE FINAL SUBMISSION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.8/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from wordcloud) (3.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from wordcloud) (1.19.1)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from wordcloud) (7.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.2.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /opt/conda/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in /opt/conda/lib/python3.8/site-packages (2.3.5)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.8/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy) (49.6.0.post20200814)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (4.48.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already up-to-date: spacy-lookups-data in /opt/conda/lib/python3.8/site-packages (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy-lookups-data) (49.6.0.post20200814)\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /opt/conda/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20200814)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!pip install -U spacy-lookups-data\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The packages used that are non-native Python are: [matplotlib](https://matplotlib.org/index.html#), [pandas](https://pandas.pydata.org), [spaCy](https://spacy.io), [seaborn](https://seaborn.pydata.org), [Gensim](https://radimrehurek.com/gensim/), [wordcloud](http://amueller.github.io/word_cloud/), and [scikit-learn](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All my imports\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import datetime\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "I did not go and scrape Twitter to get all tweets that were on climate. Instead, I found a GWU dataset of climate change tweets that were collected from 2017-2019 (Littman, Justin; Wrubel, Laura, 2019, \"Climate Change Tweets Ids\", https://doi.org/10.7910/DVN/5QCCUU, Harvard Dataverse, V1). I downloaded my sample from GWU which makes picking your criteria for the tweets much easier. You can make your own data set from all 40 million right [here](https://tweetsets.library.gwu.edu/datasets). \n",
    "\n",
    "Here is a link to my exact sample: http://tweetsets.library.gwu.edu/dataset/a66e1b6b. You can also see my sample in the form of the tweet ids in my GitHub repo: [arjungandhi521/arjungandhi521.github.io/public_data/tweets_25k.txt](https://github.com/arjungandhi521/arjungandhi521.github.io/blob/main/public_data/tweets_25k.txt). I sampled 25,000 of the 40 million tweets. The CSV I use directly below is private because of Twitter policy that one should not publish large amounts of tweets but that people in academics and such can go ahead and publisht the tweet ids. As states in the above link the data is from September 21, 2017 and May 17, 2019 and they had a gap in data collection from January 7, 2019 to April 17, 2019. I chose to exclude retweets and this sample ranges from late 2017 to mid-2019 which is the full collection time. I excluded retweets because even though a tweet with 1 billion RTs may have high influence on Twitter, having it n times in the dataset is not really going to add to my understanding of the language. I still have access to the favorites and retweets data which I will use later on. \n",
    "\n",
    "To convert each tweet ID into the actual tweet data I am using Hydrator: Hydrator [Computer Software]. Retrieved from https://github.com/docnow/hydrator. From the above repo, I downloaded [version 0.0.13 of the app](https://github.com/DocNow/hydrator/releases/tag/v0.0.13). I made a Twitter account to connect my account this Hydrator. When you download the sample just pick to download tweet ids. This will be a compressed .txt file. Just unzip it and then upload the file into Hydrator under \"Datasets\" in the desktop app. Then hit \"Add Dataset\" and then \"Start\" and then when its done you can click CSV to get the JSONL as a CSV. \n",
    "\n",
    "TALK ABOUT THEIR METHODOLOGY AND In your project talk about deletion of data when hydrated because tweets are unreachable for things like private accounts and deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>media</th>\n",
       "      <th>urls</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_listed_count</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name.1</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_time_zone</th>\n",
       "      <th>user_urls</th>\n",
       "      <th>user_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>UN cdnpoli ONpoli ABpoli</td>\n",
       "      <td>https://twitter.com/TheRebelTV/status/10722948...</td>\n",
       "      <td>https://www.therebel.media/un-global-warming-m...</td>\n",
       "      <td>91</td>\n",
       "      <td>1072294898588631040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>205531</td>\n",
       "      <td>17484</td>\n",
       "      <td>1254</td>\n",
       "      <td>Canada and the world</td>\n",
       "      <td>Rebel News</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>39001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rebelnews.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>955376892026093569</td>\n",
       "      <td>Pontifex</td>\n",
       "      <td>9.551606e+17</td>\n",
       "      <td>500704345.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Sep 17 04:42:16 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://truthout.org/articles/national-park-of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1041547863795224576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2064</td>\n",
       "      <td>2383</td>\n",
       "      <td>98</td>\n",
       "      <td>USA</td>\n",
       "      <td>OurRevolution</td>\n",
       "      <td>LeftysUnite</td>\n",
       "      <td>50006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>Spain Portugal climatechange globalwarming Hea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://news.sky.com/story/live-scorching-satu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1025728615399469058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Steven Hugh</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stevenhugh.wordpress.com/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>Resist FakePresident Dontard GOP NRA War Clima...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/mattmfm/status/93272970237...</td>\n",
       "      <td>0</td>\n",
       "      <td>932915956824682496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6198</td>\n",
       "      <td>6731</td>\n",
       "      <td>121</td>\n",
       "      <td>the beautiful \"Jemez\" USA</td>\n",
       "      <td>Athoughtz</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>155949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://TokTok.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  coordinates                      created_at  \\\n",
       "0         NaN  Tue Dec 11 01:00:00 +0000 2018   \n",
       "1         NaN  Mon Jan 22 09:49:35 +0000 2018   \n",
       "2         NaN  Mon Sep 17 04:42:16 +0000 2018   \n",
       "3         NaN  Sat Aug 04 13:02:13 +0000 2018   \n",
       "4         NaN  Tue Nov 21 10:17:51 +0000 2017   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                           UN cdnpoli ONpoli ABpoli   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Spain Portugal climatechange globalwarming Hea...   \n",
       "4  Resist FakePresident Dontard GOP NRA War Clima...   \n",
       "\n",
       "                                               media  \\\n",
       "0  https://twitter.com/TheRebelTV/status/10722948...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                urls  favorite_count  \\\n",
       "0  https://www.therebel.media/un-global-warming-m...              91   \n",
       "1                                                NaN               0   \n",
       "2  https://truthout.org/articles/national-park-of...               0   \n",
       "3  https://news.sky.com/story/live-scorching-satu...               1   \n",
       "4  https://twitter.com/mattmfm/status/93272970237...               0   \n",
       "\n",
       "                    id in_reply_to_screen_name  in_reply_to_status_id  \\\n",
       "0  1072294898588631040                     NaN                    NaN   \n",
       "1   955376892026093569                Pontifex           9.551606e+17   \n",
       "2  1041547863795224576                     NaN                    NaN   \n",
       "3  1025728615399469058                     NaN                    NaN   \n",
       "4   932915956824682496                     NaN                    NaN   \n",
       "\n",
       "   in_reply_to_user_id  ... user_followers_count user_friends_count  \\\n",
       "0                  NaN  ...               205531              17484   \n",
       "1          500704345.0  ...                    1                  4   \n",
       "2                  NaN  ...                 2064               2383   \n",
       "3                  NaN  ...                   25                 24   \n",
       "4                  NaN  ...                 6198               6731   \n",
       "\n",
       "  user_listed_count              user_location      user_name  \\\n",
       "0              1254       Canada and the world     Rebel News   \n",
       "1                 0              United States          Frank   \n",
       "2                98                        USA  OurRevolution   \n",
       "3                 1                        NaN    Steven Hugh   \n",
       "4               121  the beautiful \"Jemez\" USA      Athoughtz   \n",
       "\n",
       "   user_screen_name.1 user_statuses_count user_time_zone  \\\n",
       "0     RebelNewsOnline               39001            NaN   \n",
       "1       Frank34802901                 100            NaN   \n",
       "2         LeftysUnite               50006            NaN   \n",
       "3         Steven9Hugh                1402            NaN   \n",
       "4           athoughtz              155949            NaN   \n",
       "\n",
       "                           user_urls user_verified  \n",
       "0          https://www.rebelnews.com          True  \n",
       "1                                NaN         False  \n",
       "2                                NaN         False  \n",
       "3  https://stevenhugh.wordpress.com/         False  \n",
       "4                  http://TokTok.com         False  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/tweets_25K.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "The data set has lots of data that is not needed for this analysis. Since we are looking at sentiment over time and other factors related to polticis of a state and events, it is simplest to just drop all non-English tweets.\n",
    "\n",
    "There are lots of extranenous columns that are not relavent to this project so I just dropped them. These included geolocation data that was often missing, user information, and extranenous data about a tweet like time zone and lanague (since I drop all non-English ones to begin). These include things like user specifics like their profile details and other things like the URL of thr tweet or the language since all will be English. \n",
    "\n",
    "I followed this tutorial for help with tasks like lemmatization here and in the next section (EDA/Data Viz) making bags of words and word clouds.\n",
    "https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-English tweets and extraneous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date/time</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>91</td>\n",
       "      <td>55</td>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Mon Sep 17 04:42:02 +0000 2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          date/time  favorite_count  \\\n",
       "tweetID                                                               \n",
       "1072294898588631040  Tue Dec 11 01:00:00 +0000 2018              91   \n",
       "955376892026093569   Mon Jan 22 09:49:35 +0000 2018               0   \n",
       "1025728615399469058  Sat Aug 04 13:02:13 +0000 2018               1   \n",
       "932915956824682496   Tue Nov 21 10:17:51 +0000 2017               0   \n",
       "1041547806622797824  Mon Sep 17 04:42:02 +0000 2018               3   \n",
       "\n",
       "                     retweet_count  \\\n",
       "tweetID                              \n",
       "1072294898588631040             55   \n",
       "955376892026093569               0   \n",
       "1025728615399469058              0   \n",
       "932915956824682496               0   \n",
       "1041547806622797824              1   \n",
       "\n",
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  \n",
       "tweetID                               \n",
       "1072294898588631040  RebelNewsOnline  \n",
       "955376892026093569     Frank34802901  \n",
       "1025728615399469058      Steven9Hugh  \n",
       "932915956824682496         athoughtz  \n",
       "1041547806622797824   IndiaGreenBldg  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-English tweets from the data set\n",
    "data = data[data[\"lang\"] == \"en\"]\n",
    "\n",
    "# Drop all the unneeded columns from the data set\n",
    "cols_to_delete = [\"user_urls\", \"user_statuses_count\", \"coordinates\", \"user_name\", \"in_reply_to_status_id\", \n",
    "                  \"in_reply_to_user_id\", \"user_time_zone\", \"urls\", \"lang\", \"media\", \"source\", \n",
    "                  \"retweet_screen_name\", \"retweet_id\", \"possibly_sensitive\", \"tweet_url\",\n",
    "                  \"user_default_profile_image\", \"user_friends_count\", \"user_verified\", \"user_location\", \n",
    "                   \"in_reply_to_screen_name\", \"user_screen_name.1\",\n",
    "                  \"user_favourites_count\", \"user_listed_count\", \"user_created_at\", \"user_description\", \"place\", \n",
    "                 \"user_followers_count\", \"hashtags\"]\n",
    "\n",
    "data = data.drop(columns=cols_to_delete)\n",
    "\n",
    "# Swap the index column from 0...n to the tweet ID and rename the column from id to tweetID and rename to clarify\n",
    "# column meaning\n",
    "data = data.rename(columns={\"id\": \"tweetID\", \"created_at\": \"date/time\", \"user_screen_name\": \"tweeter\"})\n",
    "data = data.set_index('tweetID')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the likes and retweets for total interaction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date/time</th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>Tue Dec 11 01:00:00 +0000 2018</td>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>Mon Jan 22 09:49:35 +0000 2018</td>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Sat Aug 04 13:02:13 +0000 2018</td>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Tue Nov 21 10:17:51 +0000 2017</td>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Mon Sep 17 04:42:02 +0000 2018</td>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          date/time  \\\n",
       "tweetID                                               \n",
       "1072294898588631040  Tue Dec 11 01:00:00 +0000 2018   \n",
       "955376892026093569   Mon Jan 22 09:49:35 +0000 2018   \n",
       "1025728615399469058  Sat Aug 04 13:02:13 +0000 2018   \n",
       "932915956824682496   Tue Nov 21 10:17:51 +0000 2017   \n",
       "1041547806622797824  Mon Sep 17 04:42:02 +0000 2018   \n",
       "\n",
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions  \n",
       "tweetID                                                   \n",
       "1072294898588631040  RebelNewsOnline                 146  \n",
       "955376892026093569     Frank34802901                   0  \n",
       "1025728615399469058      Steven9Hugh                   1  \n",
       "932915956824682496         athoughtz                   0  \n",
       "1041547806622797824   IndiaGreenBldg                   4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the number of favorites and retweets for a tweet into an total interactions score\n",
    "total_interactions = []\n",
    "\n",
    "for row in data.iterrows():\n",
    "    tweet = row[1] \n",
    "    total = tweet[\"retweet_count\"] + tweet[\"favorite_count\"]\n",
    "    total_interactions.append(total)\n",
    "\n",
    "# Swap out the current RT and favorites columns for the total interactions columns\n",
    "data[\"total_interactions\"] = total_interactions\n",
    "data = data.drop(columns=[\"retweet_count\", \"favorite_count\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the time/date strings into date objects using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \n",
       "tweetID                                                                \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11  \n",
       "955376892026093569     Frank34802901                   0   2018-01-22  \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04  \n",
       "932915956824682496         athoughtz                   0   2017-11-21  \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dates time strings into datetime objects\n",
    "dates = []\n",
    "\n",
    "# Matching this text\n",
    "# Mon Jan 22 09:49:35 +0000 2018\n",
    "# For every row in the dataframe\n",
    "regex = re.compile(r\"(\\w{3}) (\\w{3}) (\\d\\d) (\\d\\d:\\d\\d:\\d\\d) \\+(0{4}) (\\d{4})\")\n",
    "\n",
    "# Given a string of a month return the corresponding integer for that month i.e. Jan == 1\n",
    "def numerize(str):\n",
    "    month = str.lower()\n",
    "    if (month == \"jan\"): return 1\n",
    "    elif (month == \"feb\"): return 2\n",
    "    elif (month == \"mar\"): return 3\n",
    "    elif (month == \"apr\"): return 4\n",
    "    elif (month == \"may\"): return 5\n",
    "    elif (month == \"jun\"): return 6\n",
    "    elif (month == \"jul\"): return 7 \n",
    "    elif (month == \"aug\"): return 8\n",
    "    elif (month == \"sep\"): return 9\n",
    "    elif (month == \"oct\"): return 10\n",
    "    elif (month == \"nov\"): return 11\n",
    "    elif (month == \"dec\"): return 12\n",
    "        \n",
    "for row in data.iterrows():\n",
    "    dt = row[1][\"date/time\"]\n",
    "    matches = re.search(regex, dt)\n",
    "    groups = matches.groups()    \n",
    "    month = numerize(groups[1])\n",
    "    d = datetime.date(int(groups[5]), month, int(groups[2]))\n",
    "    dates.append(d)\n",
    "    \n",
    "data = data.drop(columns=[\"date/time\"])\n",
    "data[\"date_tweeted\"] = dates\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tweets for sentiment analysis\n",
    "There are several things that need to be done to the actual text of the tweets before we can do sentiment analysis on them. To starts of, I will do some basic things like make all tweet bodies lower case so that words like CLIMATE and climate and cLiMate are all treated the same by the model I use later on. Next, I am going to remove all links from these tweets because that is irrelvanet to the sentiment of the tweet. There are many things like this that I will do here then I will move on to make the tweets \"linguistically sound\" for analysis by doing things like removing words without meaning that won't contribute the analysis and then making all words their base word or lemmatizing them.\n",
    "\n",
    "### Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \n",
       "tweetID                                                                \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11  \n",
       "955376892026093569     Frank34802901                   0   2018-01-22  \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04  \n",
       "932915956824682496         athoughtz                   0   2017-11-21  \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkless = []\n",
    "regex = re.compile(r\"http\\S+\")\n",
    "\n",
    "# remove all links from each tweet\n",
    "for row in data.iterrows():\n",
    "    txt = row[1][\"text\"]\n",
    "    if txt.find(\"https://t.co\"): \n",
    "        ll = re.sub(regex, \"\", txt)\n",
    "        linkless.append(ll)\n",
    "    else: \n",
    "        linkless.append(txt)\n",
    "\n",
    "data[\"text\"] = linkless\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Preprocess\" the tweets\n",
    "I will be using GenSim to do much of the natural langauge processing in this project. \"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\"\n",
    "\n",
    "### Tokenization\n",
    "Tokenization a fundemental first step before you do anything in code with natural language to understand it. Tokenization is taking a piece of text like \"Python is the best\" and making into a list of the words that make up that text so here that would be [\"Python\", \"is\", \"the\", \"best\"]. You can find a more in-depth explanation of tokenization here if you're so interested: https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/. Though I never explicity call something like tokenize(tweet), the preprocess function right here and the lemmatize later on are doing this. The reasons I do this twice is I want to clean up the tweet before I lemmatize (explained later) and lemmatize needs a string not a string list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[therebeltv, goes, to, two, different, un, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>[pontifex, prayers, to, god, the, one, amp, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[red, alert, in, spain, and, portugal, as, eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[trump, gop, are, the, swamp, resist, fakepres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[study, green, buildings, save, billion, in, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                                 \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11   \n",
       "955376892026093569     Frank34802901                   0   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04   \n",
       "932915956824682496         athoughtz                   0   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17   \n",
       "\n",
       "                                                                tokens  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  [therebeltv, goes, to, two, different, un, con...  \n",
       "955376892026093569   [pontifex, prayers, to, god, the, one, amp, on...  \n",
       "1025728615399469058  [red, alert, in, spain, and, portugal, as, eur...  \n",
       "932915956824682496   [trump, gop, are, the, swamp, resist, fakepres...  \n",
       "1041547806622797824  [study, green, buildings, save, billion, in, h...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "for r in data.iterrows():\n",
    "    tweets.append(gensim.utils.simple_preprocess(r[1][\"text\"]))\n",
    "    \n",
    "data[\"tokens\"] = tweets \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords and Lemmatization \n",
    "Here I will remove stopwards from the tweet bodies. These are words like \"I\" and \"this\" that add little meaning to the tweet but if left in the text will give me an innacurate depiction of the most common words in the tweets. Thne I will perform lemmatization on the tweets. This just means taking words that linguisticlly mean the same thing like walker and walking and reducing them to their base. In this case the word walk. You can read more here: https://en.wikipedia.org/wiki/Lemmatisation. I will be doing this uisng spaCy (https://spacy.io).\n",
    "\n",
    "You can find installation instructions for spaCy here: https://spacy.io/usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[therebeltv, go, different, un, conference, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>[pontifex, prayer, god, amp, true, father, jes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[red, alert, spain, portugal, europe, near, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[trump, gop, swamp, resist, fakepresident, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[study, green, building, save, billion, health...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                                 \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11   \n",
       "955376892026093569     Frank34802901                   0   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04   \n",
       "932915956824682496         athoughtz                   0   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17   \n",
       "\n",
       "                                                                tokens  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  [therebeltv, go, different, un, conference, me...  \n",
       "955376892026093569   [pontifex, prayer, god, amp, true, father, jes...  \n",
       "1025728615399469058  [red, alert, spain, portugal, europe, near, ti...  \n",
       "932915956824682496   [trump, gop, swamp, resist, fakepresident, don...  \n",
       "1041547806622797824  [study, green, building, save, billion, health...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model \n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "lemmatized = []\n",
    "# Lemmatization of tweets and stopwords removal\n",
    "for r in data.iterrows():\n",
    "    # In order to lemmatize we need to get the tweet as a string not a str list so take cleaned \n",
    "    # tokens and concat them\n",
    "    tweet = r[1][\"tokens\"]\n",
    "    text = \" \".join(tweet)\n",
    "    lemmatized.append([tok.lemma_ for tok in list(model(text)) if (tok.is_stop==False)])\n",
    "    \n",
    "data[\"tokens\"] = lemmatized\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072294898588631040</th>\n",
       "      <td>.@TheRebelTV goes to two different #UN confere...</td>\n",
       "      <td>RebelNewsOnline</td>\n",
       "      <td>146</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[therebeltv, go, different, un, conference, me...</td>\n",
       "      <td>therebeltv go different un conference medium r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955376892026093569</th>\n",
       "      <td>@Pontifex Prayers  to God the one &amp;amp; only t...</td>\n",
       "      <td>Frank34802901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>[pontifex, prayer, god, amp, true, father, jes...</td>\n",
       "      <td>pontifex prayer god amp true father jesus chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025728615399469058</th>\n",
       "      <td>Red alert in #Spain and #Portugal as Europe ne...</td>\n",
       "      <td>Steven9Hugh</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>[red, alert, spain, portugal, europe, near, ti...</td>\n",
       "      <td>red alert spain portugal europe near time heat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932915956824682496</th>\n",
       "      <td>Trump /GOP are the swamp #Resist #FakePresiden...</td>\n",
       "      <td>athoughtz</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>[trump, gop, swamp, resist, fakepresident, don...</td>\n",
       "      <td>trump gop swamp resist fakepresident dontard g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041547806622797824</th>\n",
       "      <td>Study: Green Buildings Save $6.7 Billion in #H...</td>\n",
       "      <td>IndiaGreenBldg</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[study, green, building, save, billion, health...</td>\n",
       "      <td>study green building save billion health amp c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  .@TheRebelTV goes to two different #UN confere...   \n",
       "955376892026093569   @Pontifex Prayers  to God the one &amp; only t...   \n",
       "1025728615399469058  Red alert in #Spain and #Portugal as Europe ne...   \n",
       "932915956824682496   Trump /GOP are the swamp #Resist #FakePresiden...   \n",
       "1041547806622797824  Study: Green Buildings Save $6.7 Billion in #H...   \n",
       "\n",
       "                             tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                                 \n",
       "1072294898588631040  RebelNewsOnline                 146   2018-12-11   \n",
       "955376892026093569     Frank34802901                   0   2018-01-22   \n",
       "1025728615399469058      Steven9Hugh                   1   2018-08-04   \n",
       "932915956824682496         athoughtz                   0   2017-11-21   \n",
       "1041547806622797824   IndiaGreenBldg                   4   2018-09-17   \n",
       "\n",
       "                                                                tokens  \\\n",
       "tweetID                                                                  \n",
       "1072294898588631040  [therebeltv, go, different, un, conference, me...   \n",
       "955376892026093569   [pontifex, prayer, god, amp, true, father, jes...   \n",
       "1025728615399469058  [red, alert, spain, portugal, europe, near, ti...   \n",
       "932915956824682496   [trump, gop, swamp, resist, fakepresident, don...   \n",
       "1041547806622797824  [study, green, building, save, billion, health...   \n",
       "\n",
       "                                                                 tweet  \n",
       "tweetID                                                                 \n",
       "1072294898588631040  therebeltv go different un conference medium r...  \n",
       "955376892026093569   pontifex prayer god amp true father jesus chri...  \n",
       "1025728615399469058  red alert spain portugal europe near time heat...  \n",
       "932915956824682496   trump gop swamp resist fakepresident dontard g...  \n",
       "1041547806622797824  study green building save billion health amp c...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the tweets that are one string per tweet for use later\n",
    "data[\"tweet\"]=data[\"tokens\"].apply(lambda lst: \" \".join(lst))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweeter</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>date_tweeted</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tweet</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>911502962387820544</th>\n",
       "      <td>75% of USA now ideal for disease carrying mosq...</td>\n",
       "      <td>lifelearner47</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>[usa, ideal, disease, carry, mosquito, climate...</td>\n",
       "      <td>usa ideal disease carry mosquito climatechange</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911501031904858112</th>\n",
       "      <td>With #ClimateChange there will be More Natural...</td>\n",
       "      <td>jzywick</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>[climatechange, natural, disaster]</td>\n",
       "      <td>climatechange natural disaster</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911500952628318214</th>\n",
       "      <td>#climatechange #earth \"I see more and more you...</td>\n",
       "      <td>KwameGilbert</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>[climatechange, earth, young, people, step, sa...</td>\n",
       "      <td>climatechange earth young people step say want...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911501675818569728</th>\n",
       "      <td>#ClimateChange is worrying...</td>\n",
       "      <td>jimenezroy</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>[climatechange, worry]</td>\n",
       "      <td>climatechange worry</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911504067058388992</th>\n",
       "      <td>\"The concept of global warming was created by ...</td>\n",
       "      <td>Castoropollux2</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09-23</td>\n",
       "      <td>[concept, global, warming, create, chinese, or...</td>\n",
       "      <td>concept global warming create chinese order ma...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "tweetID                                                                 \n",
       "911502962387820544  75% of USA now ideal for disease carrying mosq...   \n",
       "911501031904858112  With #ClimateChange there will be More Natural...   \n",
       "911500952628318214  #climatechange #earth \"I see more and more you...   \n",
       "911501675818569728                     #ClimateChange is worrying...    \n",
       "911504067058388992  \"The concept of global warming was created by ...   \n",
       "\n",
       "                           tweeter  total_interactions date_tweeted  \\\n",
       "tweetID                                                               \n",
       "911502962387820544   lifelearner47                   3   2017-09-23   \n",
       "911501031904858112         jzywick                   5   2017-09-23   \n",
       "911500952628318214    KwameGilbert                   1   2017-09-23   \n",
       "911501675818569728      jimenezroy                   3   2017-09-23   \n",
       "911504067058388992  Castoropollux2                   2   2017-09-23   \n",
       "\n",
       "                                                               tokens  \\\n",
       "tweetID                                                                 \n",
       "911502962387820544  [usa, ideal, disease, carry, mosquito, climate...   \n",
       "911501031904858112                 [climatechange, natural, disaster]   \n",
       "911500952628318214  [climatechange, earth, young, people, step, sa...   \n",
       "911501675818569728                             [climatechange, worry]   \n",
       "911504067058388992  [concept, global, warming, create, chinese, or...   \n",
       "\n",
       "                                                                tweet  year  \n",
       "tweetID                                                                      \n",
       "911502962387820544     usa ideal disease carry mosquito climatechange  2017  \n",
       "911501031904858112                     climatechange natural disaster  2017  \n",
       "911500952628318214  climatechange earth young people step say want...  2017  \n",
       "911501675818569728                                climatechange worry  2017  \n",
       "911504067058388992  concept global warming create chinese order ma...  2017  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to split the data frame up by year so I can visualize different time periods\n",
    "data = data.sort_values(by=[\"date_tweeted\"])\n",
    "\n",
    "# To make splitting up the data frame up by time easier, I will a year column to the tweet\n",
    "years = []\n",
    "for r in data.iterrows():\n",
    "    years.append(r[1][\"date_tweeted\"].year)\n",
    "    \n",
    "data[\"year\"] = years\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bag of words\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "\n",
    "def make_bag_of_words(tweets):\n",
    "    info = cv.fit_transform(tweets[\"tweet\"])\n",
    "    bow = pd.DataFrame(info.toarray(), columns=cv.get_feature_names())\n",
    "    bow.index=tweets.index\n",
    "    # drop the &amp column bc this is a not a word and shows up much in the word clouds and is HTML of &\n",
    "    bow.drop(columns=[\"amp\"], inplace=True)\n",
    "    return bow\n",
    "\n",
    "bags = []\n",
    "for i in range(2017,2020):\n",
    "    bags.append(make_bag_of_words(data[data[\"year\"] == i]))\n",
    "    \n",
    "# example output for 2019\n",
    "print(\"Example of what one of these bag of words looks like:\")\n",
    "bags[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a function that will display a word cloud give the word frequencies as a df\n",
    "def make_word_cloud(freqs):\n",
    "    wc = WordCloud(width=800, height=600, max_words=40,colormap=\"Dark2\").generate_from_frequencies(freqs)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Call make word cloud each bag of words summed so its one row for all tweets in that year (2017-19)\n",
    "for i in range(3): make_word_cloud(bags[i].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most interacted with tweets\n",
    "Here I will use the WordCloud library to visualize the 500 most interacted with tweets. I define an interaction as a favorite or retweet and I weighted them equally when I combined the two earlier.\n",
    "\n",
    "To make this word cloud I referenced this website: https://www.datacamp.com/community/tutorials/wordcloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a word count of the most interacted with tweets (what got the most likes and retweets)\n",
    "data = data.sort_values(by=[\"total_interactions\"], ascending=False)\n",
    "top_100 = data.head(500)\n",
    "bow_summed = make_bag_of_words(top_100).sum()\n",
    "make_word_cloud(bow_summed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During the peak times for climate on Twitter, what were people tweeting about? Was it a natural distaster, a political move, something else, or nothing special?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off by looking at december 2018\n",
    "start = datetime.date(2018, 12, 1) \n",
    "eighteen = data[data[\"year\"] == 2018]\n",
    "december_2018 = eighteen[eighteen[\"date_tweeted\"] >= start]\n",
    "bow_d18 = make_bag_of_words(december_2018)\n",
    "\n",
    "# Do the same for August 2018\n",
    "aug1 = datetime.date(2018, 8, 1) \n",
    "aug31 = datetime.date(2018, 8, 31) \n",
    "aug_on = eighteen[eighteen[\"date_tweeted\"] >= aug1]\n",
    "aug2018 = aug_on[aug_on[\"date_tweeted\"] <= aug31]\n",
    "bow_aug18 = make_bag_of_words(aug2018)\n",
    "\n",
    "# I want to see what was \"special\" about these time frames so I will remove the words that are always most popular\n",
    "popular = [\"climate\", \"change\", \"global\", \"warming\", \"climatechange\"]\n",
    "\n",
    "august = bow_aug18.drop(columns=popular).sum()\n",
    "make_word_cloud(august)\n",
    "\n",
    "december = bow_d18.drop(columns=popular).sum()\n",
    "make_word_cloud(december)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "August is in one of California's wildfire seasons according and at that time this was [the most deadly and destructive fire season in California](https://en.wikipedia.org/wiki/2018_California_wildfires) so it makes a lot of sense that we see fire related terms in the August 2018 word cloud. \n",
    "\n",
    "WAPO ARTICLE DECEMBER CLIMATE REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of Climate Tweets over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there times where people tweet about climate change more than others?\n",
    "# To make things simple lets look at the number of tweets per month year pair in the dataset\n",
    "tweets_per_month = {}\n",
    "\n",
    "# As you may recall data was collected from September 21, 2017 and May 17, 2019 and they had a gap in \n",
    "# data collection from January 7, 2019 to April 17, 2019.\n",
    "\n",
    "# Initalize the value for a tuple key for the month/year pair to 0 for all months where data was collected\n",
    "# 2017\n",
    "for i in [9, 10, 11, 12]:\n",
    "    tweets_per_month[(i,2017)] = 0\n",
    "\n",
    "# 2018\n",
    "for i in range(1,13):\n",
    "    tweets_per_month[(i,2018)] = 0\n",
    "    \n",
    "# 2019\n",
    "for i in [1, 4, 5]: \n",
    "    tweets_per_month[(i,2019)] = 0\n",
    "    \n",
    "# Iterate over data frame and add one to each tweet's proper month/year tuple's value\n",
    "for r in data.iterrows():\n",
    "    row = r[1]\n",
    "    date = row[\"date_tweeted\"]\n",
    "    tweets_per_month[(date.month, date.year)] += 1\n",
    "    \n",
    "# Remove Septembr 2017 and January-April 2019 because most during most the month data was not collected\n",
    "tweets_per_month.pop((9, 2017))\n",
    "tweets_per_month.pop((1, 2019))\n",
    "tweets_per_month.pop((4, 2019))\n",
    "# I decided to also drop May 2019 from this visualization because if I leave it then there is a 5 month gap in\n",
    "# data and this is not reflected in the plot.\n",
    "tweets_per_month.pop((5, 2019))\n",
    "\n",
    "# Convert the tuples to strings\n",
    "time = list(tweets_per_month.keys())\n",
    "count = list(tweets_per_month.values())\n",
    "\n",
    "dates = []\n",
    "for t in time:\n",
    "    dates.append(str(t[0]) + \"/\" + str(t[1])[-2:])\n",
    "    \n",
    "df = pd.DataFrame(columns = ['date', 'tweet_count'])\n",
    "df['date'] = dates\n",
    "\n",
    "df['tweet_count'] = count\n",
    "\n",
    "plt.figure(figsize=(12, 7)) #changed from 5 \n",
    "plt.plot(dates,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Length\" of Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_length']=data['tokens'].apply(lambda tokens: len(tokens))\n",
    "sns.barplot(x='year',y='tweet_length',data=data).set(title=\"Approximate Length of Tweet by Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the length of the Tweet that we are looking at here is approximate because we removed things like links (which are notably not words) and GenSim's preprocess() function removes very short and long words from the tweets to help me get the most semantically meanigful words from the tweets.\n",
    "\n",
    "When looking at this graph one major thing to consider is that it may not be such a simple oh tweet length about climate must be going up year after year for a couple of reasons. One major factor is the sample from 2018 is much larger than the other two because the majority of data colleciton took place in 2018 and much of Jan-Apr 2019 lacked sampling. In 2019, GWU collected data the first week of January, not at all in Feb-Mar, the last two weeks of April, and most of May.\n",
    "\n",
    "So roughly speaking we can say that tweet length may have increased when talking about tweets related to climate change but because of what I just said this is at best a rough guess because we aren't really saying massive changes in length for example between 2018 and 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is doing all this tweeting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who tweets about climate the most?\n",
    "tweeters = list(set(data[\"tweeter\"]))\n",
    "times_tweeted = dict.fromkeys(tweeters , 0)\n",
    "\n",
    "# Calculate how many times each tweeter has tweeted\n",
    "for r in data.iterrows():\n",
    "    row = r[1]\n",
    "    user = row[\"tweeter\"]\n",
    "    times_tweeted[user] += 1\n",
    "    \n",
    "users = list(times_tweeted.keys())\n",
    "counts = list(times_tweeted.values())\n",
    "\n",
    "df = pd.DataFrame(columns = ['user', 'tweet count'])\n",
    "df[\"user\"] = users\n",
    "df[\"tweet count\"] = counts\n",
    "df = df.sort_values(by=[\"tweet count\"], ascending=False)\n",
    "\n",
    "df = df.head(60)\n",
    "sns.displot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Sentiment Analysis\n",
    "For my section on machine learning I going to primarily focus on sentiment analysis. I will give a brief introduction to what that is but if you are interesting in learning more, I would reccommend that you check out these links that dive deeper and may explain it a bit better: https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17, https://monkeylearn.com/sentiment-analysis/.\n",
    "\n",
    "So what exactly is my goal with sentiment analysis in this section? Well to start sentiment analysis is looking at the sentiment of a tweet or really any collection of words or a word based on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
